{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c99a6f6-2c2f-487f-ab6d-38facaf43592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ECG database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ECG records: 100%|█████████████████████████████████████████████████████████████| 48/48 [00:02<00:00, 17.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 48 records from RECORDS file\n",
      "\n",
      "Global Annotation Statistics:\n",
      "symbol  count  percentage                                description\n",
      "     N  75052       66.63                                Normal beat\n",
      "     L   8075        7.17              Left bundle branch block beat\n",
      "     R   7259        6.44             Right bundle branch block beat\n",
      "     V   7130        6.33          Premature ventricular contraction\n",
      "     /   7028        6.24                                 Paced beat\n",
      "     A   2546        2.26                      Atrial premature beat\n",
      "     +   1291        1.15                              Rhythm change\n",
      "     f    982        0.87            Fusion of paced and normal beat\n",
      "     F    803        0.71      Fusion of ventricular and normal beat\n",
      "     ~    616        0.55                   Change in signal quality\n",
      "     !    472        0.42                   Ventricular flutter wave\n",
      "     \"    437        0.39                         Comment annotation\n",
      "     j    229        0.20             Nodal (junctional) escape beat\n",
      "     x    193        0.17         Non-conducted P-wave (blocked APC)\n",
      "     a    150        0.13            Aberrated atrial premature beat\n",
      "     |    132        0.12                 Isolated QRS-like artifact\n",
      "     E    106        0.09                    Ventricular escape beat\n",
      "     J     83        0.07          Nodal (junctional) premature beat\n",
      "     Q     33        0.03                        Unclassifiable beat\n",
      "     e     16        0.01                         Atrial escape beat\n",
      "     [      6        0.01  Start of ventricular flutter/fibrillation\n",
      "     ]      6        0.01    End of ventricular flutter/fibrillation\n",
      "     S      2        0.00 Supraventricular premature or ectopic beat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def get_record_list(data_dir):\n",
    "    \"\"\"\n",
    "    Read the RECORDS file to get the list of record numbers.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dir (str): Path to the directory containing the MIT-BIH database files\n",
    "    \n",
    "    Returns:\n",
    "    list: List of record numbers as strings\n",
    "    \"\"\"\n",
    "    records_file = os.path.join(data_dir, 'RECORDS')\n",
    "    with open(records_file, 'r') as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "def load_mit_bih_records(data_dir):\n",
    "    \"\"\"\n",
    "    Load all MIT-BIH Arrhythmia Database records and annotations from the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dir (str): Path to the directory containing the MIT-BIH database files\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing record information, signals, and annotations\n",
    "    \"\"\"\n",
    "    # Get record numbers from RECORDS file\n",
    "    record_numbers = get_record_list(data_dir)\n",
    "    \n",
    "    # Dictionary to store all records\n",
    "    database = {\n",
    "        'records': {},\n",
    "        'metadata': {\n",
    "            'sampling_frequency': None,\n",
    "            'total_records': 0,\n",
    "            'signal_length': None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load each record\n",
    "    for record_name in tqdm(record_numbers, desc=\"Loading ECG records\"):\n",
    "        record_path = os.path.join(data_dir, record_name)\n",
    "        \n",
    "        try:\n",
    "            # Read the record\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            \n",
    "            # Read the annotations\n",
    "            try:\n",
    "                ann = wfdb.rdann(record_path, 'atr')\n",
    "                annotations = {\n",
    "                    'sample': ann.sample,\n",
    "                    'symbol': ann.symbol,\n",
    "                    'subtype': ann.subtype,\n",
    "                    'chan': ann.chan,\n",
    "                    'num': ann.num,\n",
    "                    'aux_note': ann.aux_note,\n",
    "                    'fs': ann.fs\n",
    "                }\n",
    "                \n",
    "                # Calculate annotation statistics\n",
    "                symbol_counts = Counter(ann.symbol)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading annotations for record {record_name}: {str(e)}\")\n",
    "                annotations = None\n",
    "                symbol_counts = None\n",
    "            \n",
    "            # Store record information\n",
    "            database['records'][record_name] = {\n",
    "                'signals': record.p_signal,\n",
    "                'channels': record.sig_name,\n",
    "                'units': record.units,\n",
    "                'fs': record.fs,\n",
    "                'baseline': record.baseline,\n",
    "                'comments': record.comments,\n",
    "                'annotations': annotations,\n",
    "                'annotation_counts': symbol_counts\n",
    "            }\n",
    "            \n",
    "            # Update metadata\n",
    "            if database['metadata']['sampling_frequency'] is None:\n",
    "                database['metadata']['sampling_frequency'] = record.fs\n",
    "            if database['metadata']['signal_length'] is None:\n",
    "                database['metadata']['signal_length'] = len(record.p_signal)\n",
    "            database['metadata']['total_records'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading record {record_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return database\n",
    "\n",
    "def get_record_summary(database):\n",
    "    \"\"\"\n",
    "    Generate a summary of the loaded records including annotation statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    database (dict): The database dictionary returned by load_mit_bih_records\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Summary statistics for each record\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for record_name, record_data in database['records'].items():\n",
    "        signals = record_data['signals']\n",
    "        annotations = record_data['annotations']\n",
    "        \n",
    "        summary = {\n",
    "            'record_name': record_name,\n",
    "            'duration_seconds': len(signals) / record_data['fs'],\n",
    "            'num_channels': signals.shape[1],\n",
    "            'mean_ch1': np.mean(signals[:, 0]),\n",
    "            'std_ch1': np.std(signals[:, 0]),\n",
    "            'mean_ch2': np.mean(signals[:, 1]),\n",
    "            'std_ch2': np.std(signals[:, 1]),\n",
    "            'fs': record_data['fs'],\n",
    "            'total_annotations': len(annotations['sample']) if annotations else 0\n",
    "        }\n",
    "        \n",
    "        # Add annotation type counts if available\n",
    "        if record_data['annotation_counts']:\n",
    "            for symbol, count in record_data['annotation_counts'].items():\n",
    "                summary[f'annotation_{symbol}'] = count\n",
    "                \n",
    "        summaries.append(summary)\n",
    "    \n",
    "    return pd.DataFrame(summaries)\n",
    "\n",
    "def get_annotations_as_dataframe(record_data):\n",
    "    \"\"\"\n",
    "    Convert annotations for a single record into a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    record_data (dict): Record data dictionary containing annotations\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing all annotations with time information\n",
    "    \"\"\"\n",
    "    if not record_data['annotations']:\n",
    "        return None\n",
    "        \n",
    "    ann = record_data['annotations']\n",
    "    df = pd.DataFrame({\n",
    "        'sample': ann['sample'],\n",
    "        'time': ann['sample'] / ann['fs'],\n",
    "        'symbol': ann['symbol'],\n",
    "        'subtype': ann['subtype'],\n",
    "        'channel': ann['chan'],\n",
    "        'aux_note': ann['aux_note']\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_global_annotation_counts(database):\n",
    "    \"\"\"\n",
    "    Calculate total counts for each annotation symbol across all records.\n",
    "    \n",
    "    Parameters:\n",
    "    database (dict): The database dictionary returned by load_mit_bih_records\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with symbol counts, percentages, and descriptions\n",
    "    \"\"\"\n",
    "    # Initialize a Counter for all symbols\n",
    "    global_counts = Counter()\n",
    "    \n",
    "    # Count symbols across all records\n",
    "    for record_data in database['records'].values():\n",
    "        if record_data['annotation_counts']:\n",
    "            global_counts.update(record_data['annotation_counts'])\n",
    "    \n",
    "    # Create DataFrame with counts and percentages\n",
    "    total_annotations = sum(global_counts.values())\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'symbol': symbol,\n",
    "            'count': count,\n",
    "            'percentage': (count / total_annotations * 100),\n",
    "            'description': get_symbol_description(symbol)\n",
    "        }\n",
    "        for symbol, count in global_counts.most_common()\n",
    "    ])\n",
    "    \n",
    "    # Format percentage column\n",
    "    df['percentage'] = df['percentage'].round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_symbol_description(symbol):\n",
    "    \"\"\"\n",
    "    Get the description for each annotation symbol.\n",
    "    \n",
    "    Parameters:\n",
    "    symbol (str): The annotation symbol\n",
    "    \n",
    "    Returns:\n",
    "    str: Description of the symbol\n",
    "    \"\"\"\n",
    "    descriptions = {\n",
    "        'N': 'Normal beat',\n",
    "        'L': 'Left bundle branch block beat',\n",
    "        'R': 'Right bundle branch block beat',\n",
    "        'B': 'Bundle branch block beat (unspecified)',\n",
    "        'A': 'Atrial premature beat',\n",
    "        'a': 'Aberrated atrial premature beat',\n",
    "        'J': 'Nodal (junctional) premature beat',\n",
    "        'S': 'Supraventricular premature or ectopic beat',\n",
    "        'V': 'Premature ventricular contraction',\n",
    "        'r': 'R-on-T premature ventricular contraction',\n",
    "        'F': 'Fusion of ventricular and normal beat',\n",
    "        'e': 'Atrial escape beat',\n",
    "        'j': 'Nodal (junctional) escape beat',\n",
    "        'n': 'Supraventricular escape beat (atrial or nodal)',\n",
    "        'E': 'Ventricular escape beat',\n",
    "        '/': 'Paced beat',\n",
    "        'f': 'Fusion of paced and normal beat',\n",
    "        'Q': 'Unclassifiable beat',\n",
    "        '?': 'Beat not classified during learning',\n",
    "        '[': 'Start of ventricular flutter/fibrillation',\n",
    "        ']': 'End of ventricular flutter/fibrillation',\n",
    "        '!': 'Ventricular flutter wave',\n",
    "        'x': 'Non-conducted P-wave (blocked APC)',\n",
    "        '(': 'Waveform onset',\n",
    "        ')': 'Waveform end',\n",
    "        'p': 'Peak of P-wave',\n",
    "        't': 'Peak of T-wave',\n",
    "        'u': 'Peak of U-wave',\n",
    "        '`': 'PQ junction',\n",
    "        '\\'': 'J-point',\n",
    "        '^': 'Non-captured pacemaker artifact',\n",
    "        '|': 'Isolated QRS-like artifact',\n",
    "        '~': 'Change in signal quality',\n",
    "        '+': 'Rhythm change',\n",
    "        's': 'ST segment change',\n",
    "        'T': 'T-wave change',\n",
    "        '*': 'Systole',\n",
    "        'D': 'Diastole',\n",
    "        '=': 'Measurement annotation',\n",
    "        '\"': 'Comment annotation',\n",
    "        '@': 'Link to external data'\n",
    "    }\n",
    "    return descriptions.get(symbol, 'Unknown annotation type')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set data directory\n",
    "    data_dir = '../data/mit-bih-arrhythmia-database-1.0.0'\n",
    "    \n",
    "    # Load all records\n",
    "    print(\"Loading ECG database...\")\n",
    "    database = load_mit_bih_records(data_dir)\n",
    "    \n",
    "    # Print basic information\n",
    "    print(f\"\\nLoaded {database['metadata']['total_records']} records from RECORDS file\")\n",
    "    \n",
    "    # Get global annotation statistics\n",
    "    print(\"\\nGlobal Annotation Statistics:\")\n",
    "    annotation_stats = get_global_annotation_counts(database)\n",
    "    pd.set_option('display.max_rows', None)  # Show all rows\n",
    "    print(annotation_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040facfa-16f0-4724-9cfc-85ad371c326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|████████████████████████████████████████████████████████████████████| 38/38 [00:05<00:00,  7.02it/s]\n",
      "Loading data: 100%|████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 48184\n",
      "Validation samples: 12680\n",
      "\n",
      "Input shape: torch.Size([1, 513, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|▏                                                                    | 5/1506 [00:00<01:17, 19.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████████████████████████████████████████████████████████████| 1506/1506 [00:46<00:00, 32.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "Train Loss: 0.4385, Train Acc: 87.55%\n",
      "Val Loss: 8.7253, Val Acc: 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|                                                                     | 2/1506 [00:00<01:23, 18.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████████████████████████████████████████████████████████████| 1506/1506 [00:48<00:00, 30.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "Train Loss: 0.2660, Train Acc: 92.65%\n",
      "Val Loss: 8.5691, Val Acc: 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|▏                                                                    | 3/1506 [00:00<00:54, 27.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████████████████████████████████████████████████████████████| 1506/1506 [00:47<00:00, 31.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "Train Loss: 0.2363, Train Acc: 93.38%\n",
      "Val Loss: 10.3035, Val Acc: 0.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|▏                                                                    | 3/1506 [00:00<00:51, 28.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████████████████████████████████████████████████████████████| 1506/1506 [00:45<00:00, 33.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "Train Loss: 0.2175, Train Acc: 93.86%\n",
      "Val Loss: 11.1067, Val Acc: 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|▏                                                                    | 4/1506 [00:00<00:44, 33.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████████████████████████████████████████████████████████████| 1506/1506 [00:40<00:00, 37.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "Train Loss: 0.2065, Train Acc: 94.22%\n",
      "Val Loss: 9.8123, Val Acc: 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|▏                                                                    | 4/1506 [00:00<00:42, 35.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████████████████████████████████████████████████████████████| 1506/1506 [00:41<00:00, 36.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:\n",
      "Train Loss: 0.1978, Train Acc: 94.42%\n",
      "Val Loss: 10.8939, Val Acc: 0.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|▏                                                                    | 4/1506 [00:00<00:42, 35.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████████████████████████████████████████████████████████████| 1506/1506 [00:41<00:00, 35.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:\n",
      "Train Loss: 0.1858, Train Acc: 94.66%\n",
      "Val Loss: 9.3641, Val Acc: 0.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|▏                                                                    | 4/1506 [00:00<00:42, 35.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████████████████████████████████████████████████████████████| 1506/1506 [00:40<00:00, 37.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:\n",
      "Train Loss: 0.1805, Train Acc: 94.76%\n",
      "Val Loss: 11.1980, Val Acc: 0.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|▏                                                                    | 4/1506 [00:00<00:41, 36.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████████████████████████████████████████████████████████████| 1506/1506 [00:40<00:00, 37.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9:\n",
      "Train Loss: 0.1759, Train Acc: 95.06%\n",
      "Val Loss: 10.2643, Val Acc: 0.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|▏                                                                   | 4/1506 [00:00<00:42, 34.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|█████████████████████████████████████████████████████████████████| 1506/1506 [00:40<00:00, 36.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:\n",
      "Train Loss: 0.1711, Train Acc: 95.12%\n",
      "Val Loss: 10.5236, Val Acc: 0.22%\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class ECGSpectrogramDataset(Dataset):\n",
    "    def __init__(self, data_dir, record_numbers, window_size=1024, overlap=512):\n",
    "        self.data_dir = data_dir\n",
    "        self.record_numbers = record_numbers\n",
    "        self.window_size = window_size\n",
    "        self.overlap = overlap\n",
    "        self.spectrograms = []\n",
    "        self.labels = []\n",
    "        \n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load ECG data and create spectrograms\"\"\"\n",
    "        for record_num in tqdm(self.record_numbers, desc=\"Loading data\"):\n",
    "            # Load record and annotations\n",
    "            record_path = os.path.join(self.data_dir, str(record_num))\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            annotations = wfdb.rdann(record_path, 'atr')\n",
    "            \n",
    "            # Get signal from first channel\n",
    "            signal_data = record.p_signal[:, 0]\n",
    "            \n",
    "            # Create spectrogram\n",
    "            frequencies, times, Sxx = signal.spectrogram(\n",
    "                signal_data,\n",
    "                fs=record.fs,\n",
    "                window='hann',\n",
    "                nperseg=self.window_size,\n",
    "                noverlap=self.overlap,\n",
    "                detrend='constant'\n",
    "            )\n",
    "            \n",
    "            # Log scale spectrogram\n",
    "            Sxx = np.log1p(Sxx)\n",
    "            \n",
    "            # Normalize spectrogram\n",
    "            Sxx = (Sxx - Sxx.mean()) / (Sxx.std() + 1e-8)\n",
    "            \n",
    "            # Split spectrogram into segments and match with annotations\n",
    "            segment_duration = times[1] - times[0]\n",
    "            num_freq_bins = Sxx.shape[0]\n",
    "            \n",
    "            for i in range(len(times)):\n",
    "                time_point = times[i]\n",
    "                \n",
    "                # Find the closest annotation\n",
    "                ann_idx = np.searchsorted(annotations.sample / record.fs, time_point)\n",
    "                if ann_idx < len(annotations.symbol):\n",
    "                    label = annotations.symbol[ann_idx]\n",
    "                    \n",
    "                    # Store spectrogram segment and label\n",
    "                    self.spectrograms.append(Sxx[:, i].reshape(num_freq_bins, 1))\n",
    "                    self.labels.append(label)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        self.spectrograms = np.array(self.spectrograms)\n",
    "        \n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Shape: [num_freq_bins, 1]\n",
    "        spectrogram = torch.FloatTensor(self.spectrograms[idx])\n",
    "        label = torch.LongTensor([self.labels[idx]])[0]\n",
    "        return spectrogram.unsqueeze(0), label  # Add channel dimension [1, num_freq_bins, 1]\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=1):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block - careful with padding to handle the 1D-like input\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=(7, 3), padding=(3, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(16, 32, kernel_size=(5, 3), padding=(2, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling to handle variable input sizes\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((8, 1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, device='cuda'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (spectrograms, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(spectrograms)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            if batch_idx == 0:  # Print shapes for first batch to verify dimensions\n",
    "                print(f\"Batch shapes - Input: {spectrograms.shape}, Output: {outputs.shape}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for spectrograms, labels in val_loader:\n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                outputs = model(spectrograms)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f'\\nEpoch {epoch+1}:')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.4f}, '\n",
    "              f'Train Acc: {100.*train_correct/train_total:.2f}%')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.4f}, '\n",
    "              f'Val Acc: {100.*val_correct/val_total:.2f}%')\n",
    "        \n",
    "        # Save best model\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set data directory\n",
    "    data_dir = '../data/mit-bih-arrhythmia-database-1.0.0'\n",
    "    \n",
    "    # Read record numbers from RECORDS file\n",
    "    with open(os.path.join(data_dir, 'RECORDS'), 'r') as f:\n",
    "        record_numbers = [line.strip() for line in f]\n",
    "    \n",
    "    # Split records into train and validation sets\n",
    "    train_records, val_records = train_test_split(record_numbers, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ECGSpectrogramDataset(data_dir, train_records)\n",
    "    val_dataset = ECGSpectrogramDataset(data_dir, val_records)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_classes = len(train_dataset.label_encoder.classes_)\n",
    "    model = SimpleCNN(num_classes)\n",
    "    \n",
    "    # Print model summary\n",
    "    sample_data, _ = train_dataset[0]\n",
    "    print(f\"\\nInput shape: {sample_data.shape}\")\n",
    "    \n",
    "    # Train model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_model(model, train_loader, val_loader, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48719a98-6ccc-41f1-858b-116da9329f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
