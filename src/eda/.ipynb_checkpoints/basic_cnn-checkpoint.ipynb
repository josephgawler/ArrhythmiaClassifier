{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c99a6f6-2c2f-487f-ab6d-38facaf43592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ECG database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ECG records: 100%|█████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 48 records from RECORDS file\n",
      "\n",
      "Global Annotation Statistics:\n",
      "symbol  count  percentage                                description\n",
      "     N  75052       66.63                                Normal beat\n",
      "     L   8075        7.17              Left bundle branch block beat\n",
      "     R   7259        6.44             Right bundle branch block beat\n",
      "     V   7130        6.33          Premature ventricular contraction\n",
      "     /   7028        6.24                                 Paced beat\n",
      "     A   2546        2.26                      Atrial premature beat\n",
      "     +   1291        1.15                              Rhythm change\n",
      "     f    982        0.87            Fusion of paced and normal beat\n",
      "     F    803        0.71      Fusion of ventricular and normal beat\n",
      "     ~    616        0.55                   Change in signal quality\n",
      "     !    472        0.42                   Ventricular flutter wave\n",
      "     \"    437        0.39                         Comment annotation\n",
      "     j    229        0.20             Nodal (junctional) escape beat\n",
      "     x    193        0.17         Non-conducted P-wave (blocked APC)\n",
      "     a    150        0.13            Aberrated atrial premature beat\n",
      "     |    132        0.12                 Isolated QRS-like artifact\n",
      "     E    106        0.09                    Ventricular escape beat\n",
      "     J     83        0.07          Nodal (junctional) premature beat\n",
      "     Q     33        0.03                        Unclassifiable beat\n",
      "     e     16        0.01                         Atrial escape beat\n",
      "     [      6        0.01  Start of ventricular flutter/fibrillation\n",
      "     ]      6        0.01    End of ventricular flutter/fibrillation\n",
      "     S      2        0.00 Supraventricular premature or ectopic beat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_record_list(data_dir):\n",
    "    \"\"\"\n",
    "    Read the RECORDS file to get the list of record numbers.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dir (str): Path to the directory containing the MIT-BIH database files\n",
    "    \n",
    "    Returns:\n",
    "    list: List of record numbers as strings\n",
    "    \"\"\"\n",
    "    records_file = os.path.join(data_dir, 'RECORDS')\n",
    "    with open(records_file, 'r') as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "def load_mit_bih_records(data_dir):\n",
    "    \"\"\"\n",
    "    Load all MIT-BIH Arrhythmia Database records and annotations from the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dir (str): Path to the directory containing the MIT-BIH database files\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing record information, signals, and annotations\n",
    "    \"\"\"\n",
    "    # Get record numbers from RECORDS file\n",
    "    record_numbers = get_record_list(data_dir)\n",
    "    \n",
    "    # Dictionary to store all records\n",
    "    database = {\n",
    "        'records': {},\n",
    "        'metadata': {\n",
    "            'sampling_frequency': None,\n",
    "            'total_records': 0,\n",
    "            'signal_length': None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load each record\n",
    "    for record_name in tqdm(record_numbers, desc=\"Loading ECG records\"):\n",
    "        record_path = os.path.join(data_dir, record_name)\n",
    "        \n",
    "        try:\n",
    "            # Read the record\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            \n",
    "            # Read the annotations\n",
    "            try:\n",
    "                ann = wfdb.rdann(record_path, 'atr')\n",
    "                annotations = {\n",
    "                    'sample': ann.sample,\n",
    "                    'symbol': ann.symbol,\n",
    "                    'subtype': ann.subtype,\n",
    "                    'chan': ann.chan,\n",
    "                    'num': ann.num,\n",
    "                    'aux_note': ann.aux_note,\n",
    "                    'fs': ann.fs\n",
    "                }\n",
    "                \n",
    "                # Calculate annotation statistics\n",
    "                symbol_counts = Counter(ann.symbol)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading annotations for record {record_name}: {str(e)}\")\n",
    "                annotations = None\n",
    "                symbol_counts = None\n",
    "            \n",
    "            # Store record information\n",
    "            database['records'][record_name] = {\n",
    "                'signals': record.p_signal,\n",
    "                'channels': record.sig_name,\n",
    "                'units': record.units,\n",
    "                'fs': record.fs,\n",
    "                'baseline': record.baseline,\n",
    "                'comments': record.comments,\n",
    "                'annotations': annotations,\n",
    "                'annotation_counts': symbol_counts\n",
    "            }\n",
    "            \n",
    "            # Update metadata\n",
    "            if database['metadata']['sampling_frequency'] is None:\n",
    "                database['metadata']['sampling_frequency'] = record.fs\n",
    "            if database['metadata']['signal_length'] is None:\n",
    "                database['metadata']['signal_length'] = len(record.p_signal)\n",
    "            database['metadata']['total_records'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading record {record_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return database\n",
    "\n",
    "def get_record_summary(database):\n",
    "    \"\"\"\n",
    "    Generate a summary of the loaded records including annotation statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    database (dict): The database dictionary returned by load_mit_bih_records\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Summary statistics for each record\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for record_name, record_data in database['records'].items():\n",
    "        signals = record_data['signals']\n",
    "        annotations = record_data['annotations']\n",
    "        \n",
    "        summary = {\n",
    "            'record_name': record_name,\n",
    "            'duration_seconds': len(signals) / record_data['fs'],\n",
    "            'num_channels': signals.shape[1],\n",
    "            'mean_ch1': np.mean(signals[:, 0]),\n",
    "            'std_ch1': np.std(signals[:, 0]),\n",
    "            'mean_ch2': np.mean(signals[:, 1]),\n",
    "            'std_ch2': np.std(signals[:, 1]),\n",
    "            'fs': record_data['fs'],\n",
    "            'total_annotations': len(annotations['sample']) if annotations else 0\n",
    "        }\n",
    "        \n",
    "        # Add annotation type counts if available\n",
    "        if record_data['annotation_counts']:\n",
    "            for symbol, count in record_data['annotation_counts'].items():\n",
    "                summary[f'annotation_{symbol}'] = count\n",
    "                \n",
    "        summaries.append(summary)\n",
    "    \n",
    "    return pd.DataFrame(summaries)\n",
    "\n",
    "def get_annotations_as_dataframe(record_data):\n",
    "    \"\"\"\n",
    "    Convert annotations for a single record into a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    record_data (dict): Record data dictionary containing annotations\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing all annotations with time information\n",
    "    \"\"\"\n",
    "    if not record_data['annotations']:\n",
    "        return None\n",
    "        \n",
    "    ann = record_data['annotations']\n",
    "    df = pd.DataFrame({\n",
    "        'sample': ann['sample'],\n",
    "        'time': ann['sample'] / ann['fs'],\n",
    "        'symbol': ann['symbol'],\n",
    "        'subtype': ann['subtype'],\n",
    "        'channel': ann['chan'],\n",
    "        'aux_note': ann['aux_note']\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_global_annotation_counts(database):\n",
    "    \"\"\"\n",
    "    Calculate total counts for each annotation symbol across all records.\n",
    "    \n",
    "    Parameters:\n",
    "    database (dict): The database dictionary returned by load_mit_bih_records\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with symbol counts, percentages, and descriptions\n",
    "    \"\"\"\n",
    "    # Initialize a Counter for all symbols\n",
    "    global_counts = Counter()\n",
    "    \n",
    "    # Count symbols across all records\n",
    "    for record_data in database['records'].values():\n",
    "        if record_data['annotation_counts']:\n",
    "            global_counts.update(record_data['annotation_counts'])\n",
    "    \n",
    "    # Create DataFrame with counts and percentages\n",
    "    total_annotations = sum(global_counts.values())\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'symbol': symbol,\n",
    "            'count': count,\n",
    "            'percentage': (count / total_annotations * 100),\n",
    "            'description': get_symbol_description(symbol)\n",
    "        }\n",
    "        for symbol, count in global_counts.most_common()\n",
    "    ])\n",
    "    \n",
    "    # Format percentage column\n",
    "    df['percentage'] = df['percentage'].round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_symbol_description(symbol):\n",
    "    \"\"\"\n",
    "    Get the description for each annotation symbol.\n",
    "    \n",
    "    Parameters:\n",
    "    symbol (str): The annotation symbol\n",
    "    \n",
    "    Returns:\n",
    "    str: Description of the symbol\n",
    "    \"\"\"\n",
    "    descriptions = {\n",
    "        'N': 'Normal beat',\n",
    "        'L': 'Left bundle branch block beat',\n",
    "        'R': 'Right bundle branch block beat',\n",
    "        'B': 'Bundle branch block beat (unspecified)',\n",
    "        'A': 'Atrial premature beat',\n",
    "        'a': 'Aberrated atrial premature beat',\n",
    "        'J': 'Nodal (junctional) premature beat',\n",
    "        'S': 'Supraventricular premature or ectopic beat',\n",
    "        'V': 'Premature ventricular contraction',\n",
    "        'r': 'R-on-T premature ventricular contraction',\n",
    "        'F': 'Fusion of ventricular and normal beat',\n",
    "        'e': 'Atrial escape beat',\n",
    "        'j': 'Nodal (junctional) escape beat',\n",
    "        'n': 'Supraventricular escape beat (atrial or nodal)',\n",
    "        'E': 'Ventricular escape beat',\n",
    "        '/': 'Paced beat',\n",
    "        'f': 'Fusion of paced and normal beat',\n",
    "        'Q': 'Unclassifiable beat',\n",
    "        '?': 'Beat not classified during learning',\n",
    "        '[': 'Start of ventricular flutter/fibrillation',\n",
    "        ']': 'End of ventricular flutter/fibrillation',\n",
    "        '!': 'Ventricular flutter wave',\n",
    "        'x': 'Non-conducted P-wave (blocked APC)',\n",
    "        '(': 'Waveform onset',\n",
    "        ')': 'Waveform end',\n",
    "        'p': 'Peak of P-wave',\n",
    "        't': 'Peak of T-wave',\n",
    "        'u': 'Peak of U-wave',\n",
    "        '`': 'PQ junction',\n",
    "        '\\'': 'J-point',\n",
    "        '^': 'Non-captured pacemaker artifact',\n",
    "        '|': 'Isolated QRS-like artifact',\n",
    "        '~': 'Change in signal quality',\n",
    "        '+': 'Rhythm change',\n",
    "        's': 'ST segment change',\n",
    "        'T': 'T-wave change',\n",
    "        '*': 'Systole',\n",
    "        'D': 'Diastole',\n",
    "        '=': 'Measurement annotation',\n",
    "        '\"': 'Comment annotation',\n",
    "        '@': 'Link to external data'\n",
    "    }\n",
    "    return descriptions.get(symbol, 'Unknown annotation type')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set data directory\n",
    "    data_dir = '../data/mit-bih-arrhythmia-database-1.0.0'\n",
    "    \n",
    "    # Load all records\n",
    "    print(\"Loading ECG database...\")\n",
    "    database = load_mit_bih_records(data_dir)\n",
    "    \n",
    "    # Print basic information\n",
    "    print(f\"\\nLoaded {database['metadata']['total_records']} records from RECORDS file\")\n",
    "    \n",
    "    # Get global annotation statistics\n",
    "    print(\"\\nGlobal Annotation Statistics:\")\n",
    "    annotation_stats = get_global_annotation_counts(database)\n",
    "    pd.set_option('display.max_rows', None)  # Show all rows\n",
    "    print(annotation_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "040facfa-16f0-4724-9cfc-85ad371c326a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|████████████████████████████████████████████████████████████████████| 38/38 [00:11<00:00,  3.27it/s]\n",
      "Loading data: 100%|████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 48184\n",
      "Validation samples: 12680\n",
      "\n",
      "Input shape: torch.Size([1, 513, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|▏                                                                    | 3/1506 [00:00<01:56, 12.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████████████████████████████████████████████████████████████| 1506/1506 [01:32<00:00, 16.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "Train Loss: 0.4309, Train Acc: 87.76%\n",
      "Val Loss: 8.5396, Val Acc: 0.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|                                                                     | 2/1506 [00:00<01:32, 16.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████████████████████████████████████████████████████████████| 1506/1506 [01:31<00:00, 16.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "Train Loss: 0.2620, Train Acc: 92.73%\n",
      "Val Loss: 10.7438, Val Acc: 0.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|                                                                     | 2/1506 [00:00<01:37, 15.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████████████████████████████████████████████████████████████| 1506/1506 [01:32<00:00, 16.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "Train Loss: 0.2353, Train Acc: 93.38%\n",
      "Val Loss: 11.0026, Val Acc: 0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|                                                                     | 2/1506 [00:00<01:30, 16.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   4%|██▍                                                                 | 55/1506 [00:03<01:26, 16.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 213\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    212\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 213\u001b[0m train_model(model, train_loader, val_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[7], line 140\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[0;32m    138\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(spectrograms)\n\u001b[0;32m    139\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m--> 140\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    141\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    143\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\apnea\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\apnea\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\apnea\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class ECGSpectrogramDataset(Dataset):\n",
    "    def __init__(self, data_dir, record_numbers, window_size=1024, overlap=512):\n",
    "        self.data_dir = data_dir\n",
    "        self.record_numbers = record_numbers\n",
    "        self.window_size = window_size\n",
    "        self.overlap = overlap\n",
    "        self.spectrograms = []\n",
    "        self.labels = []\n",
    "        \n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load ECG data and create spectrograms\"\"\"\n",
    "        for record_num in tqdm(self.record_numbers, desc=\"Loading data\"):\n",
    "            # Load record and annotations\n",
    "            record_path = os.path.join(self.data_dir, str(record_num))\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            annotations = wfdb.rdann(record_path, 'atr')\n",
    "            \n",
    "            # Get signal from first channel\n",
    "            signal_data = record.p_signal[:, 0]\n",
    "            \n",
    "            # Create spectrogram\n",
    "            frequencies, times, Sxx = signal.spectrogram(\n",
    "                signal_data,\n",
    "                fs=record.fs,\n",
    "                window='hann',\n",
    "                nperseg=self.window_size,\n",
    "                noverlap=self.overlap,\n",
    "                detrend='constant'\n",
    "            )\n",
    "            \n",
    "            # Log scale spectrogram\n",
    "            Sxx = np.log1p(Sxx)\n",
    "            \n",
    "            # Normalize spectrogram\n",
    "            Sxx = (Sxx - Sxx.mean()) / (Sxx.std() + 1e-8)\n",
    "            \n",
    "            # Split spectrogram into segments and match with annotations\n",
    "            segment_duration = times[1] - times[0]\n",
    "            num_freq_bins = Sxx.shape[0]\n",
    "            \n",
    "            for i in range(len(times)):\n",
    "                time_point = times[i]\n",
    "                \n",
    "                # Find the closest annotation\n",
    "                ann_idx = np.searchsorted(annotations.sample / record.fs, time_point)\n",
    "                if ann_idx < len(annotations.symbol):\n",
    "                    label = annotations.symbol[ann_idx]\n",
    "                    \n",
    "                    # Store spectrogram segment and label\n",
    "                    self.spectrograms.append(Sxx[:, i].reshape(num_freq_bins, 1))\n",
    "                    self.labels.append(label)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        self.spectrograms = np.array(self.spectrograms)\n",
    "        \n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Shape: [num_freq_bins, 1]\n",
    "        spectrogram = torch.FloatTensor(self.spectrograms[idx])\n",
    "        label = torch.LongTensor([self.labels[idx]])[0]\n",
    "        return spectrogram.unsqueeze(0), label  # Add channel dimension [1, num_freq_bins, 1]\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=1):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block - careful with padding to handle the 1D-like input\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=(7, 3), padding=(3, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(16, 32, kernel_size=(5, 3), padding=(2, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling to handle variable input sizes\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((8, 1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, device='cuda'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (spectrograms, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(spectrograms)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            if batch_idx == 0:  # Print shapes for first batch to verify dimensions\n",
    "                print(f\"Batch shapes - Input: {spectrograms.shape}, Output: {outputs.shape}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for spectrograms, labels in val_loader:\n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                outputs = model(spectrograms)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f'\\nEpoch {epoch+1}:')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.4f}, '\n",
    "              f'Train Acc: {100.*train_correct/train_total:.2f}%')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.4f}, '\n",
    "              f'Val Acc: {100.*val_correct/val_total:.2f}%')\n",
    "        \n",
    "        # Save best model\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set data directory\n",
    "    data_dir = '../data/mit-bih-arrhythmia-database-1.0.0'\n",
    "    \n",
    "    # Read record numbers from RECORDS file\n",
    "    with open(os.path.join(data_dir, 'RECORDS'), 'r') as f:\n",
    "        record_numbers = [line.strip() for line in f]\n",
    "\n",
    "    # Count the frequency of annotations in each record\n",
    "    record_frequencies = {}\n",
    "    for record_num in record_numbers:\n",
    "        record_path = os.path.join(data_dir, str(record_num))\n",
    "        try:\n",
    "            annotations = wfdb.rdann(record_path, 'atr')\n",
    "            record_frequencies[record_num] = len(annotations.symbol)  # Count of annotations\n",
    "        except:\n",
    "            continue  # Skip if annotation file is missing\n",
    "\n",
    "    # Select top 5 most frequent records\n",
    "    top_records = [record for record, _ in Counter(record_frequencies).most_common(5)]\n",
    "\n",
    "    # Split records into train and validation sets\n",
    "    train_records, val_records = train_test_split(top_records, test_size=0.2, random_state=42)\n",
    "        \n",
    "    # Create datasets\n",
    "    train_dataset = ECGSpectrogramDataset(data_dir, train_records)\n",
    "    val_dataset = ECGSpectrogramDataset(data_dir, val_records)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_classes = len(train_dataset.label_encoder.classes_)\n",
    "    model = SimpleCNN(num_classes)\n",
    "    \n",
    "    # Print model summary\n",
    "    sample_data, _ = train_dataset[0]\n",
    "    print(f\"\\nInput shape: {sample_data.shape}\")\n",
    "    \n",
    "    # Train model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_model(model, train_loader, val_loader, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c536dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.67it/s]\n",
      "Loading data: 100%|██████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5072\n",
      "Validation samples: 1268\n",
      "\n",
      "Input shape: torch.Size([1, 513, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▉                                                                     | 2/159 [00:00<00:11, 14.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████████████████████████████████████████████| 159/159 [00:09<00:00, 16.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "Train Loss: 0.3674, Train Acc: 90.20%\n",
      "Val Loss: 1.8043, Val Acc: 81.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   1%|▉                                                                     | 2/159 [00:00<00:08, 17.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████| 159/159 [00:09<00:00, 16.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "Train Loss: 0.2421, Train Acc: 92.59%\n",
      "Val Loss: 1.6483, Val Acc: 81.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   1%|▉                                                                     | 2/159 [00:00<00:09, 16.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████| 159/159 [00:09<00:00, 16.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "Train Loss: 0.2208, Train Acc: 93.77%\n",
      "Val Loss: 1.6065, Val Acc: 81.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   1%|▉                                                                     | 2/159 [00:00<00:10, 15.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████| 159/159 [00:09<00:00, 17.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "Train Loss: 0.1994, Train Acc: 93.95%\n",
      "Val Loss: 1.8520, Val Acc: 81.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   1%|▉                                                                     | 2/159 [00:00<00:09, 16.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████| 159/159 [00:09<00:00, 16.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "Train Loss: 0.1997, Train Acc: 94.34%\n",
      "Val Loss: 1.4827, Val Acc: 79.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   1%|▉                                                                     | 2/159 [00:00<00:08, 17.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████████████████████████████████████████████| 159/159 [00:09<00:00, 16.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:\n",
      "Train Loss: 0.1892, Train Acc: 94.72%\n",
      "Val Loss: 1.7665, Val Acc: 80.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   1%|▉                                                                     | 2/159 [00:00<00:10, 15.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████████████████████████████████████████████| 159/159 [00:09<00:00, 16.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:\n",
      "Train Loss: 0.1821, Train Acc: 94.72%\n",
      "Val Loss: 1.2518, Val Acc: 71.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   1%|▉                                                                     | 2/159 [00:00<00:08, 18.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████████████████████████████████████████████| 159/159 [00:09<00:00, 16.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:\n",
      "Train Loss: 0.1659, Train Acc: 94.85%\n",
      "Val Loss: 1.4601, Val Acc: 78.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   1%|▉                                                                     | 2/159 [00:00<00:09, 16.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████████████████████████████████████████████| 159/159 [00:09<00:00, 17.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9:\n",
      "Train Loss: 0.1680, Train Acc: 95.11%\n",
      "Val Loss: 1.8065, Val Acc: 80.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   1%|▊                                                                    | 2/159 [00:00<00:09, 16.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([32, 1, 513, 1]), Output: torch.Size([32, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████████████████████████████████████████████| 159/159 [00:09<00:00, 16.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:\n",
      "Train Loss: 0.1569, Train Acc: 95.70%\n",
      "Val Loss: 1.7002, Val Acc: 74.05%\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EnhancedECGSpectrogramDataset(Dataset):\n",
    "    def __init__(self, data_dir, record_numbers, window_size=2048, overlap=1024):\n",
    "        self.data_dir = data_dir\n",
    "        self.record_numbers = record_numbers\n",
    "        self.window_size = window_size\n",
    "        self.overlap = overlap\n",
    "        self.spectrograms = []\n",
    "        self.labels = []\n",
    "        \n",
    "    def _load_data(self):\n",
    "        for record_num in tqdm(self.record_numbers, desc=\"Loading data\"):\n",
    "            record_path = os.path.join(self.data_dir, str(record_num))\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            annotations = wfdb.rdann(record_path, 'atr')\n",
    "            \n",
    "            # Use both channels instead of just first channel\n",
    "            for channel in range(record.p_signal.shape[1]):\n",
    "                signal_data = record.p_signal[:, channel]\n",
    "                \n",
    "                # Apply bandpass filter to remove noise\n",
    "                nyquist = record.fs / 2\n",
    "                low = 0.5 / nyquist\n",
    "                high = 50.0 / nyquist\n",
    "                b, a = signal.butter(4, [low, high], btype='band')\n",
    "                filtered_signal = signal.filtfilt(b, a, signal_data)\n",
    "                \n",
    "                # Create spectrogram with modified parameters\n",
    "                frequencies, times, Sxx = signal.spectrogram(\n",
    "                    filtered_signal,\n",
    "                    fs=record.fs,\n",
    "                    window='hamming',  # Changed from hann\n",
    "                    nperseg=self.window_size,\n",
    "                    noverlap=self.overlap,\n",
    "                    detrend='linear',  # Changed from constant\n",
    "                    scaling='density'  # Added scaling parameter\n",
    "                )\n",
    "                \n",
    "                # Apply more sophisticated normalization\n",
    "                Sxx = np.log1p(Sxx)  # Log scale\n",
    "                Sxx = (Sxx - np.percentile(Sxx, 1)) / (np.percentile(Sxx, 99) - np.percentile(Sxx, 1))\n",
    "                Sxx = np.clip(Sxx, 0, 1)  # Clip outliers\n",
    "                \n",
    "                # Include context windows\n",
    "                context_size = 3\n",
    "                for i in range(context_size, len(times) - context_size):\n",
    "                    context_window = Sxx[:, i-context_size:i+context_size+1]\n",
    "                    \n",
    "                    # Find the closest annotation\n",
    "                    time_point = times[i]\n",
    "                    ann_idx = np.searchsorted(annotations.sample / record.fs, time_point)\n",
    "                    if ann_idx < len(annotations.symbol):\n",
    "                        label = annotations.symbol[ann_idx]\n",
    "                        self.spectrograms.append(context_window)\n",
    "                        self.labels.append(label)\n",
    "\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=1):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        \n",
    "        # Residual block definition\n",
    "        def residual_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        self.features = nn.ModuleList([\n",
    "            # Initial convolution\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(input_channels, 32, kernel_size=7, padding=3),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2)\n",
    "            ),\n",
    "            \n",
    "            # Residual blocks with increasing channels\n",
    "            residual_block(32, 32),\n",
    "            residual_block(32, 64),\n",
    "            residual_block(64, 128),\n",
    "            \n",
    "            # SE block for channel attention\n",
    "            nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(128, 64, kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply feature extraction with residual connections\n",
    "        identity = x\n",
    "        for i, layer in enumerate(self.features[:-1]):  # Exclude SE block\n",
    "            x = layer(x)\n",
    "            if i > 0 and i % 2 == 0:  # Add residual every 2 blocks\n",
    "                if identity.shape == x.shape:\n",
    "                    x += identity\n",
    "                identity = x\n",
    "        \n",
    "        # Apply SE block\n",
    "        se_weights = self.features[-1](x)\n",
    "        x = x * se_weights\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = self.global_pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_model_with_scheduler(model, train_loader, val_loader, num_epochs=20, device='cuda'):\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Added label smoothing\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    \n",
    "    # Cosine annealing scheduler with warm restarts\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=5, T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Initialize mixup\n",
    "    alpha = 0.2\n",
    "    \n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for spectrograms, labels in train_loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            \n",
    "            # Apply mixup\n",
    "            if np.random.random() > 0.5:\n",
    "                lam = np.random.beta(alpha, alpha)\n",
    "                index = torch.randperm(spectrograms.size(0)).to(device)\n",
    "                mixed_spectrograms = lam * spectrograms + (1 - lam) * spectrograms[index]\n",
    "                \n",
    "                outputs = model(mixed_spectrograms)\n",
    "                loss = lam * criterion(outputs, labels) + (1 - lam) * criterion(outputs, labels[index])\n",
    "            else:\n",
    "                outputs = model(spectrograms)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation logic remains the same...\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for spectrograms, labels in val_loader:\n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                outputs = model(spectrograms)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f'\\nEpoch {epoch+1}:')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.4f}, '\n",
    "              f'Train Acc: {100.*train_correct/train_total:.2f}%')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.4f}, '\n",
    "              f'Val Acc: {100.*val_correct/val_total:.2f}%')\n",
    "        \n",
    "        # Save best model\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set data directory\n",
    "    data_dir = '../data/mit-bih-arrhythmia-database-1.0.0'\n",
    "    \n",
    "    # Read record numbers from RECORDS file\n",
    "    with open(os.path.join(data_dir, 'RECORDS'), 'r') as f:\n",
    "        record_numbers = [line.strip() for line in f]\n",
    "\n",
    "    # Count the frequency of annotations in each record\n",
    "    record_frequencies = {}\n",
    "    for record_num in record_numbers:\n",
    "        record_path = os.path.join(data_dir, str(record_num))\n",
    "        try:\n",
    "            annotations = wfdb.rdann(record_path, 'atr')\n",
    "            record_frequencies[record_num] = len(annotations.symbol)  # Count of annotations\n",
    "        except:\n",
    "            continue  # Skip if annotation file is missing\n",
    "\n",
    "    # Select top 5 most frequent records\n",
    "    top_records = [record for record, _ in Counter(record_frequencies).most_common(5)]\n",
    "\n",
    "    # Split records into train and validation sets\n",
    "    train_records, val_records = train_test_split(top_records, test_size=0.2, random_state=42)\n",
    "        \n",
    "    # Create datasets\n",
    "    train_dataset = ECGSpectrogramDataset(data_dir, train_records)\n",
    "    val_dataset = ECGSpectrogramDataset(data_dir, val_records)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_classes = len(train_dataset.label_encoder.classes_)\n",
    "    model = SimpleCNN(num_classes)\n",
    "    \n",
    "    # Print model summary\n",
    "    sample_data, _ = train_dataset[0]\n",
    "    print(f\"\\nInput shape: {sample_data.shape}\")\n",
    "    \n",
    "    # Train model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_model(model, train_loader, val_loader, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f15f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing predictions: 100%|███████████████████████████████████████████████████████████| 40/40 [00:01<00:00, 37.08it/s]\n",
      "C:\\Users\\julia\\anaconda3\\envs\\apnea\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\julia\\anaconda3\\envs\\apnea\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\julia\\anaconda3\\envs\\apnea\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 2 elements, new values have 3 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 180\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# Assuming model, data_loader, and label_encoder are defined\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 180\u001b[0m     results \u001b[38;5;241m=\u001b[39m analyze_model_predictions(model, val_loader, device, train_dataset\u001b[38;5;241m.\u001b[39mlabel_encoder)\n\u001b[0;32m    181\u001b[0m     print_analysis_results(results)\n",
      "Cell \u001b[1;32mIn[9], line 45\u001b[0m, in \u001b[0;36manalyze_model_predictions\u001b[1;34m(model, data_loader, device, label_encoder)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m incorrect_indices:\n\u001b[0;32m     37\u001b[0m                 incorrect_samples\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     38\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_label\u001b[39m\u001b[38;5;124m'\u001b[39m: labels[idx]\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m     39\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_label\u001b[39m\u001b[38;5;124m'\u001b[39m: preds[idx]\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_idx\u001b[39m\u001b[38;5;124m'\u001b[39m: idx\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     43\u001b[0m                 })\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m analyze_results(all_preds, all_labels, all_probs, incorrect_samples, label_encoder)\n",
      "Cell \u001b[1;32mIn[9], line 108\u001b[0m, in \u001b[0;36manalyze_results\u001b[1;34m(all_preds, all_labels, all_probs, incorrect_samples, label_encoder)\u001b[0m\n\u001b[0;32m    102\u001b[0m     confusion_pairs\u001b[38;5;241m.\u001b[39mappend((true_class, pred_class))\n\u001b[0;32m    104\u001b[0m confusion_counts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m    105\u001b[0m     pd\u001b[38;5;241m.\u001b[39mSeries(confusion_pairs)\u001b[38;5;241m.\u001b[39mvalue_counts(),\n\u001b[0;32m    106\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    107\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m--> 108\u001b[0m confusion_counts\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# 5. Generate summary statistics\u001b[39;00m\n\u001b[0;32m    111\u001b[0m summary_stats \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverall Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMost Confused Pairs\u001b[39m\u001b[38;5;124m'\u001b[39m: confusion_counts\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m     }\n\u001b[0;32m    127\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\apnea\\Lib\\site-packages\\pandas\\core\\generic.py:6313\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   6311\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   6312\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[1;32m-> 6313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[0;32m   6314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m   6315\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mproperties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\apnea\\Lib\\site-packages\\pandas\\core\\generic.py:814\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    813\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[1;32m--> 814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mset_axis(axis, labels)\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\apnea\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:238\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_set_axis(axis, new_labels)\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\apnea\\Lib\\site-packages\\pandas\\core\\internals\\base.py:98\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 2 elements, new values have 3 elements"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_model_predictions(model, data_loader, device, label_encoder):\n",
    "    \"\"\"\n",
    "    Analyze model predictions and generate detailed diagnostics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    incorrect_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (spectrograms, labels) in enumerate(tqdm(data_loader, desc=\"Analyzing predictions\")):\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probabilities.cpu().numpy())\n",
    "            \n",
    "            # Store incorrect predictions for analysis\n",
    "            incorrect_mask = preds != labels\n",
    "            if incorrect_mask.any():\n",
    "                incorrect_indices = torch.where(incorrect_mask)[0]\n",
    "                for idx in incorrect_indices:\n",
    "                    incorrect_samples.append({\n",
    "                        'true_label': labels[idx].item(),\n",
    "                        'predicted_label': preds[idx].item(),\n",
    "                        'confidence': probabilities[idx][preds[idx]].item(),\n",
    "                        'batch_idx': batch_idx,\n",
    "                        'sample_idx': idx.item()\n",
    "                    })\n",
    "    \n",
    "    return analyze_results(all_preds, all_labels, all_probs, incorrect_samples, label_encoder)\n",
    "\n",
    "def analyze_results(all_preds, all_labels, all_probs, incorrect_samples, label_encoder):\n",
    "    \"\"\"\n",
    "    Generate comprehensive analysis of model predictions.\n",
    "    \"\"\"\n",
    "    # Convert numerical labels to original classes\n",
    "    class_names = label_encoder.classes_\n",
    "    true_classes = [class_names[label] for label in all_labels]\n",
    "    pred_classes = [class_names[pred] for pred in all_preds]\n",
    "    \n",
    "    # 1. Generate confusion matrix\n",
    "    cm = confusion_matrix(true_classes, pred_classes)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Generate classification report\n",
    "    report = classification_report(true_classes, pred_classes, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # 3. Analyze prediction confidence\n",
    "    confidence_analysis = {\n",
    "        'correct_conf': [],\n",
    "        'incorrect_conf': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(all_preds)):\n",
    "        conf = all_probs[i][all_preds[i]]\n",
    "        if all_preds[i] == all_labels[i]:\n",
    "            confidence_analysis['correct_conf'].append(conf)\n",
    "        else:\n",
    "            confidence_analysis['incorrect_conf'].append(conf)\n",
    "    \n",
    "    # Plot confidence distributions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(confidence_analysis['correct_conf'], alpha=0.5, label='Correct', bins=50)\n",
    "    plt.hist(confidence_analysis['incorrect_conf'], alpha=0.5, label='Incorrect', bins=50)\n",
    "    plt.title('Prediction Confidence Distribution')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confidence_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Analyze most common confusion pairs\n",
    "    confusion_pairs = []\n",
    "    for sample in incorrect_samples:\n",
    "        true_class = class_names[sample['true_label']]\n",
    "        pred_class = class_names[sample['predicted_label']]\n",
    "        confusion_pairs.append((true_class, pred_class))\n",
    "    \n",
    "    confusion_counts = pd.DataFrame(\n",
    "        pd.Series(confusion_pairs).value_counts(),\n",
    "        columns=['count']\n",
    "    ).reset_index()\n",
    "    confusion_counts.columns = ['True Class', 'Predicted Class', 'Count']\n",
    "    \n",
    "    # 5. Generate summary statistics\n",
    "    summary_stats = {\n",
    "        'Overall Accuracy': report['accuracy'],\n",
    "        'Most Confused Pairs': confusion_counts.head(5).to_dict('records'),\n",
    "        'Per Class Performance': {\n",
    "            class_name: {\n",
    "                'precision': report[class_name]['precision'],\n",
    "                'recall': report[class_name]['recall'],\n",
    "                'f1-score': report[class_name]['f1-score'],\n",
    "                'support': report[class_name]['support']\n",
    "            }\n",
    "            for class_name in class_names\n",
    "        },\n",
    "        'Average Confidence': {\n",
    "            'Correct Predictions': np.mean(confidence_analysis['correct_conf']),\n",
    "            'Incorrect Predictions': np.mean(confidence_analysis['incorrect_conf'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report_df,\n",
    "        'confidence_analysis': confidence_analysis,\n",
    "        'confusion_pairs': confusion_counts,\n",
    "        'summary_stats': summary_stats,\n",
    "        'incorrect_samples': incorrect_samples\n",
    "    }\n",
    "\n",
    "def print_analysis_results(results):\n",
    "    \"\"\"\n",
    "    Print formatted analysis results.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== ECG Classification Model Analysis ===\\n\")\n",
    "    \n",
    "    # Overall Performance\n",
    "    print(\"Overall Performance:\")\n",
    "    print(f\"Accuracy: {results['summary_stats']['Overall Accuracy']:.4f}\")\n",
    "    print(\"\\n=== Per-Class Performance ===\")\n",
    "    print(results['classification_report'])\n",
    "    \n",
    "    # Most Common Confusion Pairs\n",
    "    print(\"\\nTop 5 Most Common Confusion Pairs:\")\n",
    "    for pair in results['summary_stats']['Most Confused Pairs']:\n",
    "        print(f\"True: {pair['True Class']} → Predicted: {pair['Predicted Class']} \"\n",
    "              f\"(Count: {pair['Count']})\")\n",
    "    \n",
    "    # Confidence Analysis\n",
    "    print(\"\\nConfidence Analysis:\")\n",
    "    print(f\"Average confidence for correct predictions: \"\n",
    "          f\"{results['summary_stats']['Average Confidence']['Correct Predictions']:.4f}\")\n",
    "    print(f\"Average confidence for incorrect predictions: \"\n",
    "          f\"{results['summary_stats']['Average Confidence']['Incorrect Predictions']:.4f}\")\n",
    "    \n",
    "    # Examples of High-Confidence Mistakes\n",
    "    print(\"\\nHigh-Confidence Mistakes:\")\n",
    "    high_conf_mistakes = sorted(\n",
    "        [x for x in results['incorrect_samples'] if x['confidence'] > 0.8],\n",
    "        key=lambda x: x['confidence'],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "    \n",
    "    for mistake in high_conf_mistakes:\n",
    "        print(f\"True: {mistake['true_label']} → Predicted: {mistake['predicted_label']} \"\n",
    "              f\"(Confidence: {mistake['confidence']:.4f})\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming model, data_loader, and label_encoder are defined\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    results = analyze_model_predictions(model, val_loader, device, train_dataset.label_encoder)\n",
    "    print_analysis_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4aaac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
