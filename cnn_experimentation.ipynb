{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f802a34",
   "metadata": {},
   "source": [
    "## DS4440 Midterm Project: CNN Model Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "256eb9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68214d05",
   "metadata": {},
   "source": [
    "First, create a database of raw ECG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7272db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ECG records: 100%|██████████| 48/48 [00:02<00:00, 21.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 48 records from RECORDS file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_record_list(data_dir):\n",
    "    \"\"\"\n",
    "    Read the RECORDS file to get the list of record numbers.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dir (str): Path to the directory containing the MIT-BIH database files\n",
    "    \n",
    "    Returns:\n",
    "    list: List of record numbers as strings\n",
    "    \"\"\"\n",
    "    records_file = os.path.join(data_dir, 'RECORDS')\n",
    "    with open(records_file, 'r') as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "def load_mit_bih_records(data_dir):\n",
    "    \"\"\"\n",
    "    Load all MIT-BIH Arrhythmia Database records and annotations from the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dir (str): Path to the directory containing the MIT-BIH database files\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing record information, signals, and annotations\n",
    "    \"\"\"\n",
    "    # Get record numbers from RECORDS file\n",
    "    record_numbers = get_record_list(data_dir)\n",
    "    \n",
    "    # Dictionary to store all records\n",
    "    database = {\n",
    "        'records': {},\n",
    "        'metadata': {\n",
    "            'sampling_frequency': None,\n",
    "            'total_records': 0,\n",
    "            'signal_length': None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load each record\n",
    "    for record_name in tqdm(record_numbers, desc=\"Loading ECG records\"):\n",
    "        record_path = os.path.join(data_dir, record_name)\n",
    "        \n",
    "        try:\n",
    "            # Read the record\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            \n",
    "            # Read the annotations\n",
    "            try:\n",
    "                ann = wfdb.rdann(record_path, 'atr')\n",
    "                annotations = {\n",
    "                    'sample': ann.sample,\n",
    "                    'symbol': ann.symbol,\n",
    "                    'subtype': ann.subtype,\n",
    "                    'chan': ann.chan,\n",
    "                    'num': ann.num,\n",
    "                    'aux_note': ann.aux_note,\n",
    "                    'fs': ann.fs\n",
    "                }\n",
    "                \n",
    "                # Calculate annotation statistics\n",
    "                symbol_counts = Counter(ann.symbol)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading annotations for record {record_name}: {str(e)}\")\n",
    "                annotations = None\n",
    "                symbol_counts = None\n",
    "            \n",
    "            # Store record information\n",
    "            database['records'][record_name] = {\n",
    "                'signals': record.p_signal,\n",
    "                'channels': record.sig_name,\n",
    "                'units': record.units,\n",
    "                'fs': record.fs,\n",
    "                'baseline': record.baseline,\n",
    "                'comments': record.comments,\n",
    "                'annotations': annotations,\n",
    "                'annotation_counts': symbol_counts\n",
    "            }\n",
    "            \n",
    "            # Update metadata\n",
    "            if database['metadata']['sampling_frequency'] is None:\n",
    "                database['metadata']['sampling_frequency'] = record.fs\n",
    "            if database['metadata']['signal_length'] is None:\n",
    "                database['metadata']['signal_length'] = len(record.p_signal)\n",
    "            database['metadata']['total_records'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading record {record_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return database\n",
    "\n",
    "data_dir = 'src/data/mit-bih-arrhythmia-database-1.0.0'\n",
    "database = load_mit_bih_records(data_dir)\n",
    "print(f\"\\nLoaded {database['metadata']['total_records']} records from RECORDS file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37580a6",
   "metadata": {},
   "source": [
    "Get statistics of label distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96eeb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global Annotation Statistics:\n",
      "symbol  count  percentage                       description\n",
      "     N  75052       66.63                       Normal beat\n",
      "     L   8075        7.17          Left bundle branch block\n",
      "     R   7259        6.44         Right bundle branch block\n",
      "     V   7130        6.33 Premature ventricular contraction\n",
      "     /   7028        6.24                        Paced beat\n",
      "     A   2546        2.26             Atrial premature beat\n",
      "     +   1291        1.15                     Rhythm change\n",
      "     f    982        0.87                Unknown annotation\n",
      "     F    803        0.71                Unknown annotation\n",
      "     ~    616        0.55             Signal quality change\n",
      "     !    472        0.42                Unknown annotation\n",
      "     \"    437        0.39                Unknown annotation\n",
      "     j    229        0.20                Unknown annotation\n",
      "     x    193        0.17                Unknown annotation\n",
      "     a    150        0.13                Unknown annotation\n",
      "     |    132        0.12                Unknown annotation\n",
      "     E    106        0.09                Unknown annotation\n",
      "     J     83        0.07                Unknown annotation\n",
      "     Q     33        0.03               Unclassifiable beat\n",
      "     e     16        0.01                Unknown annotation\n",
      "     [      6        0.01                Unknown annotation\n",
      "     ]      6        0.01                Unknown annotation\n",
      "     S      2        0.00                Unknown annotation\n"
     ]
    }
   ],
   "source": [
    "def get_record_summary(database):\n",
    "    \"\"\"Generate a summary of loaded records with annotation statistics.\"\"\"\n",
    "    summaries = []\n",
    "\n",
    "    for record_name, record_data in database['records'].items():\n",
    "        signals, annotations = record_data['signals'], record_data['annotations']\n",
    "        summary = {\n",
    "            'record_name': record_name,\n",
    "            'duration_seconds': len(signals) / record_data['fs'],\n",
    "            'num_channels': signals.shape[1],\n",
    "            'fs': record_data['fs'],\n",
    "            'total_annotations': len(annotations['sample']) if annotations else 0\n",
    "        }\n",
    "\n",
    "        # Channel statistics\n",
    "        for i in range(signals.shape[1]):\n",
    "            summary.update({f'mean_ch{i + 1}': np.mean(signals[:, i]), f'std_ch{i + 1}': np.std(signals[:, i])})\n",
    "\n",
    "        # Annotation counts\n",
    "        if record_data['annotation_counts']:\n",
    "            summary.update({f'annotation_{sym}': cnt for sym, cnt in record_data['annotation_counts'].items()})\n",
    "\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return pd.DataFrame(summaries)\n",
    "\n",
    "def get_annotations_as_dataframe(record_data):\n",
    "    \"\"\"Convert record annotations to a pandas DataFrame.\"\"\"\n",
    "    if not record_data['annotations']:\n",
    "        return None\n",
    "\n",
    "    ann = record_data['annotations']\n",
    "    return pd.DataFrame({\n",
    "        'sample': ann['sample'],\n",
    "        'time': ann['sample'] / ann['fs'],\n",
    "        'symbol': ann['symbol'],\n",
    "        'subtype': ann['subtype'],\n",
    "        'channel': ann['chan'],\n",
    "        'aux_note': ann['aux_note']\n",
    "    })\n",
    "\n",
    "def get_global_annotation_counts(database):\n",
    "    \"\"\"Calculate global counts of annotation symbols.\"\"\"\n",
    "    global_counts = Counter()\n",
    "\n",
    "    for record_data in database['records'].values():\n",
    "        global_counts.update(record_data.get('annotation_counts', {}))\n",
    "\n",
    "    total = sum(global_counts.values())\n",
    "    return pd.DataFrame([\n",
    "        {'symbol': sym, 'count': cnt, 'percentage': round(cnt / total * 100, 2), 'description': get_symbol_description(sym)}\n",
    "        for sym, cnt in global_counts.most_common()\n",
    "    ])\n",
    "\n",
    "def get_symbol_description(symbol):\n",
    "    \"\"\"Return the description of an annotation symbol.\"\"\"\n",
    "    descriptions = {\n",
    "        'N': 'Normal beat', 'L': 'Left bundle branch block', 'R': 'Right bundle branch block',\n",
    "        'A': 'Atrial premature beat', 'V': 'Premature ventricular contraction', '/': 'Paced beat',\n",
    "        'Q': 'Unclassifiable beat', '+': 'Rhythm change', '~': 'Signal quality change'\n",
    "    }\n",
    "    return descriptions.get(symbol, 'Unknown annotation')\n",
    "\n",
    "# Display global annotation statistics\n",
    "print(\"\\nGlobal Annotation Statistics:\")\n",
    "annotation_stats = get_global_annotation_counts(database)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(annotation_stats.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40be635",
   "metadata": {},
   "source": [
    "Get training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a967ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mECGDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples, labels):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m samples\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(labels)\n",
    "        \n",
    "        # Print distribution\n",
    "        label_counts = Counter(self.labels)\n",
    "        print(\"\\nDataset label distribution:\")\n",
    "        for label_idx, count in label_counts.items():\n",
    "            label_name = self.label_encoder.inverse_transform([label_idx])[0]\n",
    "            print(f\"Label {label_name}: {count} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.FloatTensor(self.samples[idx])\n",
    "        label = torch.LongTensor([self.labels[idx]])[0]\n",
    "        return sample.unsqueeze(0), label\n",
    "    \n",
    "def prepare_data(data_dir):\n",
    "    \"\"\"Prepare ECG datasets with upsampling of 'A' and 'V' labels.\"\"\"\n",
    "    \n",
    "    REMOVE_LABELS = {'+', '~', '\"', '[', ']', '|'}\n",
    "    all_samples = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Read RECORDS file for all available patient recordings\n",
    "    with open(os.path.join(data_dir, 'RECORDS'), 'r') as f:\n",
    "        record_numbers = [line.strip() for line in f]\n",
    "\n",
    "    label_counter = Counter()\n",
    "    \n",
    "    # Count occurrences of each label\n",
    "    for record_num in tqdm(record_numbers, desc=\"Counting labels\"):\n",
    "        record_path = os.path.join(data_dir, str(record_num))\n",
    "        try:\n",
    "            annotations = wfdb.rdann(record_path, 'atr')\n",
    "            label_counter.update(annotations.symbol)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Remove unwanted labels and calculate the 1% threshold\n",
    "    total_beats = sum(label_counter.values())\n",
    "    min_threshold = total_beats * 0.01  # 1% threshold\n",
    "\n",
    "    valid_labels = {label for label, count in label_counter.items() if count >= min_threshold and label not in REMOVE_LABELS}\n",
    "    print(f\"Selected Labels (≥1% frequency): {valid_labels}\")\n",
    "\n",
    "    # Process data and extract spectrograms\n",
    "    for record_num in tqdm(record_numbers, desc=\"Processing data\"):\n",
    "        record_path = os.path.join(data_dir, str(record_num))\n",
    "        try:\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            annotations = wfdb.rdann(record_path, 'atr')\n",
    "            signal_data = record.p_signal[:, 0]\n",
    "            \n",
    "            frequencies, times, Sxx = signal.spectrogram(signal_data, fs=record.fs, window='hann',\n",
    "                                                         nperseg=1024, noverlap=512, detrend='constant')\n",
    "            \n",
    "            Sxx = np.log1p(Sxx)\n",
    "            Sxx = (Sxx - Sxx.mean()) / (Sxx.std() + 1e-8)\n",
    "\n",
    "            for i in range(len(times)):\n",
    "                time_point = times[i]\n",
    "                ann_idx = np.searchsorted(annotations.sample / record.fs, time_point)\n",
    "                if ann_idx < len(annotations.symbol):\n",
    "                    label = annotations.symbol[ann_idx]\n",
    "                    if label in valid_labels:\n",
    "                        all_samples.append(Sxx[:, i].reshape(Sxx.shape[0], 1))\n",
    "                        all_labels.append(label)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    all_samples = np.array(all_samples)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Train-Validation-Test Split\n",
    "    indices = np.arange(len(all_labels))\n",
    "    train_idx, temp_idx = train_test_split(indices, test_size=0.3, stratify=all_labels)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=all_labels[temp_idx])\n",
    "\n",
    "    train_samples = all_samples[train_idx]\n",
    "    train_labels = all_labels[train_idx]\n",
    "    val_samples = all_samples[val_idx]\n",
    "    val_labels = all_labels[val_idx]\n",
    "    test_samples = all_samples[test_idx]\n",
    "    test_labels = all_labels[test_idx]\n",
    "\n",
    "    train_dataset = ECGDataset(train_samples, train_labels)\n",
    "    val_dataset = ECGDataset(val_samples, val_labels)\n",
    "    test_dataset = ECGDataset(test_samples, test_labels)\n",
    "\n",
    "\n",
    "    # Upsample 'A' and 'V' labels in the training set\n",
    "    upsampled_samples = []\n",
    "    upsampled_labels = []\n",
    "\n",
    "    label_encoder = train_dataset.label_encoder  # Keep the same encoder\n",
    "\n",
    "    a_idx = label_encoder.transform(['A'])[0]\n",
    "    v_idx = label_encoder.transform(['V'])[0]\n",
    "\n",
    "    a_count = sum(train_dataset.labels == a_idx)\n",
    "    v_count = sum(train_dataset.labels == v_idx)\n",
    "\n",
    "    max_count = max(a_count, v_count)\n",
    "\n",
    "    for label_idx in [a_idx, v_idx]:\n",
    "        label_samples = train_dataset.samples[train_dataset.labels == label_idx]\n",
    "        current_count = len(label_samples)\n",
    "        \n",
    "        if current_count < max_count:\n",
    "            num_to_duplicate = max_count - current_count\n",
    "            duplicate_indices = np.random.choice(len(label_samples), size=num_to_duplicate, replace=True)\n",
    "            duplicated_samples = label_samples[duplicate_indices]\n",
    "\n",
    "            upsampled_samples.extend(duplicated_samples)\n",
    "            upsampled_labels.extend([label_encoder.inverse_transform([label_idx])[0]] * num_to_duplicate)\n",
    "\n",
    "    upsampled_samples = np.array(upsampled_samples)\n",
    "    upsampled_labels = np.array(upsampled_labels)\n",
    "    upsampled_labels_encoded = label_encoder.transform(upsampled_labels)\n",
    "\n",
    "    final_train_samples = np.concatenate([train_dataset.samples, upsampled_samples])\n",
    "    final_train_labels = np.concatenate([train_dataset.labels, upsampled_labels_encoded])\n",
    "    final_train_dataset = ECGDataset(final_train_samples, final_train_labels)\n",
    "\n",
    "    return final_train_dataset, val_dataset, test_dataset\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = prepare_data(data_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test sampples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d14a5",
   "metadata": {},
   "source": [
    "Create CNN model, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bc8e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=1):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=(9, 5), padding=(4, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d((4, 1)),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=(5, 3), padding=(2, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((8, 1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70cc0ffb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input shape: torch.Size([1, 513, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████████████████████████████████████████████████████████████| 1285/1285 [00:55<00:00, 23.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Training Accuracy: 76.48%\n",
      "Validation Accuracy: 88.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████████████████████████████████████████████████████████████| 1285/1285 [00:59<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10\n",
      "Training Accuracy: 85.68%\n",
      "Validation Accuracy: 89.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████████████████████████████████████████████████████████████| 1285/1285 [01:06<00:00, 19.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10\n",
      "Training Accuracy: 87.45%\n",
      "Validation Accuracy: 92.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████████████████████████████████████████████████████████████| 1285/1285 [01:52<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10\n",
      "Training Accuracy: 88.31%\n",
      "Validation Accuracy: 92.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████████████████████████████████████████████████████████████| 1285/1285 [01:53<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10\n",
      "Training Accuracy: 89.19%\n",
      "Validation Accuracy: 92.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████████████████████████████████████████████████████████████| 1285/1285 [01:57<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10\n",
      "Training Accuracy: 89.97%\n",
      "Validation Accuracy: 92.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████████████████████████████████████████████████████████████| 1285/1285 [02:02<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10\n",
      "Training Accuracy: 90.15%\n",
      "Validation Accuracy: 92.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████████████████████████████████████████████████████████████| 1285/1285 [03:29<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10\n",
      "Training Accuracy: 91.00%\n",
      "Validation Accuracy: 94.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████████████████████████████████████████████████████████████| 1285/1285 [02:41<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10\n",
      "Training Accuracy: 91.22%\n",
      "Validation Accuracy: 88.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|█████████████████████████████████████████████████████████████████| 1285/1285 [02:16<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10\n",
      "Training Accuracy: 91.70%\n",
      "Validation Accuracy: 94.06%\n",
      "\n",
      "Evaluating on Test Set...\n",
      "\n",
      "Overall Test Accuracy: 94.39%\n",
      "\n",
      "Per-class Test Accuracy:\n",
      "Class /: 99.03%\n",
      "Class A: 91.23%\n",
      "Class L: 98.12%\n",
      "Class N: 93.33%\n",
      "Class R: 98.16%\n",
      "Class V: 91.94%\n"
     ]
    }
   ],
   "source": [
    "def compute_class_weights(labels):\n",
    "    \"\"\"Compute class weights based on the inverse frequency of each class.\"\"\"\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    num_classes = len(class_counts)\n",
    "    \n",
    "    weights = {cls: total_samples / (num_classes * count) for cls, count in class_counts.items()}\n",
    "    \n",
    "    # Convert to tensor\n",
    "    class_weights = torch.tensor([weights[i] for i in range(num_classes)], dtype=torch.float)\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, test_loader, num_epochs=10, device='cuda'):\n",
    "    all_labels = [label.item() for _, label in train_loader.dataset]\n",
    "    class_weights = compute_class_weights(all_labels).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        \n",
    "        for spectrograms, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(spectrograms)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        train_accuracy = 100. * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for spectrograms, labels in val_loader:\n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                outputs = model(spectrograms)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                val_correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_accuracy = 100. * val_correct / val_total\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    # Test phase\n",
    "    print(\"\\nEvaluating on Test Set...\")\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    class_correct = Counter()\n",
    "    class_total = Counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels in test_loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            predictions = outputs.argmax(1)\n",
    "            test_correct += predictions.eq(labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            for pred, true in zip(predictions, labels):\n",
    "                if pred == true:\n",
    "                    class_correct[true.item()] += 1\n",
    "                class_total[true.item()] += 1\n",
    "    \n",
    "    test_accuracy = 100. * test_correct / test_total\n",
    "    print(f\"\\nOverall Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # Print per-class accuracy\n",
    "    print(\"\\nPer-class Test Accuracy:\")\n",
    "    for class_idx in range(len(train_loader.dataset.label_encoder.classes_)):\n",
    "        class_name = train_loader.dataset.label_encoder.inverse_transform([class_idx])[0]\n",
    "        if class_total[class_idx] > 0:\n",
    "            class_acc = 100. * class_correct[class_idx] / class_total[class_idx]\n",
    "            print(f\"Class {class_name}: {class_acc:.2f}%\")\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(train_dataset.label_encoder.classes_)\n",
    "model = SimpleCNN(num_classes)\n",
    "\n",
    "# Print model summary\n",
    "sample_data, _ = train_dataset[0]\n",
    "print(f\"\\nInput shape: {sample_data.shape}\")\n",
    "\n",
    "# Train model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_model(model, train_loader, val_loader, test_loader, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9405773e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved to prediction_analysis.png\n"
     ]
    }
   ],
   "source": [
    "def visualize_predictions(model, test_loader, device, save_path='prediction_analysis.png'):\n",
    "    \"\"\"\n",
    "    Visualize one correct and one incorrect prediction for each class.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        test_loader: DataLoader containing test data\n",
    "        device: Device to run model on\n",
    "        save_path: Path to save the visualization\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Dictionary to store examples for each class\n",
    "    # Structure: {class_idx: {'correct': (spectrogram, true_label), 'incorrect': (spectrogram, pred_label)}}\n",
    "    examples = {}\n",
    "    num_classes = len(test_loader.dataset.label_encoder.classes_)\n",
    "    \n",
    "    # Initialize dictionary for all classes\n",
    "    for i in range(num_classes):\n",
    "        examples[i] = {'correct': None, 'incorrect': None}\n",
    "    \n",
    "    # Collect examples\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels in test_loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            predictions = outputs.argmax(1)\n",
    "            \n",
    "            # Move tensors to CPU and convert to numpy for visualization\n",
    "            spectrograms = spectrograms.cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            predictions = predictions.cpu().numpy()\n",
    "            \n",
    "            # For each sample in the batch\n",
    "            for i in range(len(labels)):\n",
    "                true_label = labels[i]\n",
    "                pred_label = predictions[i]\n",
    "                \n",
    "                # If we haven't found a correct prediction for this class yet\n",
    "                if predictions[i] == labels[i] and examples[true_label]['correct'] is None:\n",
    "                    examples[true_label]['correct'] = (spectrograms[i], pred_label)\n",
    "                \n",
    "                # If we haven't found an incorrect prediction for this class yet\n",
    "                if predictions[i] != labels[i] and examples[true_label]['incorrect'] is None:\n",
    "                    examples[true_label]['incorrect'] = (spectrograms[i], pred_label)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(15, 3 * num_classes))\n",
    "    plt.suptitle('Correct vs Incorrect Predictions per Class', fontsize=16)\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        class_name = test_loader.dataset.label_encoder.inverse_transform([class_idx])[0]\n",
    "        \n",
    "        # Plot correct prediction\n",
    "        if examples[class_idx]['correct'] is not None:\n",
    "            plt.subplot(num_classes, 2, 2 * class_idx + 1)\n",
    "            spec, pred = examples[class_idx]['correct']\n",
    "            plt.imshow(spec[0], aspect='auto', origin='lower')\n",
    "            plt.title(f'{class_name}\\nCorrect Prediction', fontsize=10)\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Frequency')\n",
    "        \n",
    "        # Plot incorrect prediction\n",
    "        if examples[class_idx]['incorrect'] is not None:\n",
    "            plt.subplot(num_classes, 2, 2 * class_idx + 2)\n",
    "            spec, pred = examples[class_idx]['incorrect']\n",
    "            plt.imshow(spec[0], aspect='auto', origin='lower')\n",
    "            predicted_class = test_loader.dataset.label_encoder.inverse_transform([pred])[0]\n",
    "            plt.title(f'{class_name}\\nIncorrect Prediction (Predicted: {predicted_class})', fontsize=10)\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Visualization saved to {save_path}\")\n",
    "\n",
    "# Load the best model and visualize predictions\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "visualize_predictions(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273bb71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
