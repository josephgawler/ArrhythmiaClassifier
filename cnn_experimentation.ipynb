{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e96eeb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ECG database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ECG records: 100%|█████████████████████████████████████████████████████████████| 48/48 [00:05<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 48 records from RECORDS file\n",
      "\n",
      "Global Annotation Statistics:\n",
      "symbol  count  percentage                                description\n",
      "     N  75052       66.63                                Normal beat\n",
      "     L   8075        7.17              Left bundle branch block beat\n",
      "     R   7259        6.44             Right bundle branch block beat\n",
      "     V   7130        6.33          Premature ventricular contraction\n",
      "     /   7028        6.24                                 Paced beat\n",
      "     A   2546        2.26                      Atrial premature beat\n",
      "     +   1291        1.15                              Rhythm change\n",
      "     f    982        0.87            Fusion of paced and normal beat\n",
      "     F    803        0.71      Fusion of ventricular and normal beat\n",
      "     ~    616        0.55                   Change in signal quality\n",
      "     !    472        0.42                   Ventricular flutter wave\n",
      "     \"    437        0.39                         Comment annotation\n",
      "     j    229        0.20             Nodal (junctional) escape beat\n",
      "     x    193        0.17         Non-conducted P-wave (blocked APC)\n",
      "     a    150        0.13            Aberrated atrial premature beat\n",
      "     |    132        0.12                 Isolated QRS-like artifact\n",
      "     E    106        0.09                    Ventricular escape beat\n",
      "     J     83        0.07          Nodal (junctional) premature beat\n",
      "     Q     33        0.03                        Unclassifiable beat\n",
      "     e     16        0.01                         Atrial escape beat\n",
      "     [      6        0.01  Start of ventricular flutter/fibrillation\n",
      "     ]      6        0.01    End of ventricular flutter/fibrillation\n",
      "     S      2        0.00 Supraventricular premature or ectopic beat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_record_list(data_dir):\n",
    "    \"\"\"\n",
    "    Read the RECORDS file to get the list of record numbers.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dir (str): Path to the directory containing the MIT-BIH database files\n",
    "    \n",
    "    Returns:\n",
    "    list: List of record numbers as strings\n",
    "    \"\"\"\n",
    "    records_file = os.path.join(data_dir, 'RECORDS')\n",
    "    with open(records_file, 'r') as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "def load_mit_bih_records(data_dir):\n",
    "    \"\"\"\n",
    "    Load all MIT-BIH Arrhythmia Database records and annotations from the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dir (str): Path to the directory containing the MIT-BIH database files\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing record information, signals, and annotations\n",
    "    \"\"\"\n",
    "    # Get record numbers from RECORDS file\n",
    "    record_numbers = get_record_list(data_dir)\n",
    "    \n",
    "    # Dictionary to store all records\n",
    "    database = {\n",
    "        'records': {},\n",
    "        'metadata': {\n",
    "            'sampling_frequency': None,\n",
    "            'total_records': 0,\n",
    "            'signal_length': None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load each record\n",
    "    for record_name in tqdm(record_numbers, desc=\"Loading ECG records\"):\n",
    "        record_path = os.path.join(data_dir, record_name)\n",
    "        \n",
    "        try:\n",
    "            # Read the record\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            \n",
    "            # Read the annotations\n",
    "            try:\n",
    "                ann = wfdb.rdann(record_path, 'atr')\n",
    "                annotations = {\n",
    "                    'sample': ann.sample,\n",
    "                    'symbol': ann.symbol,\n",
    "                    'subtype': ann.subtype,\n",
    "                    'chan': ann.chan,\n",
    "                    'num': ann.num,\n",
    "                    'aux_note': ann.aux_note,\n",
    "                    'fs': ann.fs\n",
    "                }\n",
    "                \n",
    "                # Calculate annotation statistics\n",
    "                symbol_counts = Counter(ann.symbol)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading annotations for record {record_name}: {str(e)}\")\n",
    "                annotations = None\n",
    "                symbol_counts = None\n",
    "            \n",
    "            # Store record information\n",
    "            database['records'][record_name] = {\n",
    "                'signals': record.p_signal,\n",
    "                'channels': record.sig_name,\n",
    "                'units': record.units,\n",
    "                'fs': record.fs,\n",
    "                'baseline': record.baseline,\n",
    "                'comments': record.comments,\n",
    "                'annotations': annotations,\n",
    "                'annotation_counts': symbol_counts\n",
    "            }\n",
    "            \n",
    "            # Update metadata\n",
    "            if database['metadata']['sampling_frequency'] is None:\n",
    "                database['metadata']['sampling_frequency'] = record.fs\n",
    "            if database['metadata']['signal_length'] is None:\n",
    "                database['metadata']['signal_length'] = len(record.p_signal)\n",
    "            database['metadata']['total_records'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading record {record_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return database\n",
    "\n",
    "def get_record_summary(database):\n",
    "    \"\"\"\n",
    "    Generate a summary of the loaded records including annotation statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    database (dict): The database dictionary returned by load_mit_bih_records\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Summary statistics for each record\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for record_name, record_data in database['records'].items():\n",
    "        signals = record_data['signals']\n",
    "        annotations = record_data['annotations']\n",
    "        \n",
    "        summary = {\n",
    "            'record_name': record_name,\n",
    "            'duration_seconds': len(signals) / record_data['fs'],\n",
    "            'num_channels': signals.shape[1],\n",
    "            'mean_ch1': np.mean(signals[:, 0]),\n",
    "            'std_ch1': np.std(signals[:, 0]),\n",
    "            'mean_ch2': np.mean(signals[:, 1]),\n",
    "            'std_ch2': np.std(signals[:, 1]),\n",
    "            'fs': record_data['fs'],\n",
    "            'total_annotations': len(annotations['sample']) if annotations else 0\n",
    "        }\n",
    "        \n",
    "        # Add annotation type counts if available\n",
    "        if record_data['annotation_counts']:\n",
    "            for symbol, count in record_data['annotation_counts'].items():\n",
    "                summary[f'annotation_{symbol}'] = count\n",
    "                \n",
    "        summaries.append(summary)\n",
    "    \n",
    "    return pd.DataFrame(summaries)\n",
    "\n",
    "def get_annotations_as_dataframe(record_data):\n",
    "    \"\"\"\n",
    "    Convert annotations for a single record into a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    record_data (dict): Record data dictionary containing annotations\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing all annotations with time information\n",
    "    \"\"\"\n",
    "    if not record_data['annotations']:\n",
    "        return None\n",
    "        \n",
    "    ann = record_data['annotations']\n",
    "    df = pd.DataFrame({\n",
    "        'sample': ann['sample'],\n",
    "        'time': ann['sample'] / ann['fs'],\n",
    "        'symbol': ann['symbol'],\n",
    "        'subtype': ann['subtype'],\n",
    "        'channel': ann['chan'],\n",
    "        'aux_note': ann['aux_note']\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_global_annotation_counts(database):\n",
    "    \"\"\"\n",
    "    Calculate total counts for each annotation symbol across all records.\n",
    "    \n",
    "    Parameters:\n",
    "    database (dict): The database dictionary returned by load_mit_bih_records\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with symbol counts, percentages, and descriptions\n",
    "    \"\"\"\n",
    "    # Initialize a Counter for all symbols\n",
    "    global_counts = Counter()\n",
    "    \n",
    "    # Count symbols across all records\n",
    "    for record_data in database['records'].values():\n",
    "        if record_data['annotation_counts']:\n",
    "            global_counts.update(record_data['annotation_counts'])\n",
    "    \n",
    "    # Create DataFrame with counts and percentages\n",
    "    total_annotations = sum(global_counts.values())\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'symbol': symbol,\n",
    "            'count': count,\n",
    "            'percentage': (count / total_annotations * 100),\n",
    "            'description': get_symbol_description(symbol)\n",
    "        }\n",
    "        for symbol, count in global_counts.most_common()\n",
    "    ])\n",
    "    \n",
    "    # Format percentage column\n",
    "    df['percentage'] = df['percentage'].round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_symbol_description(symbol):\n",
    "    \"\"\"\n",
    "    Get the description for each annotation symbol.\n",
    "    \n",
    "    Parameters:\n",
    "    symbol (str): The annotation symbol\n",
    "    \n",
    "    Returns:\n",
    "    str: Description of the symbol\n",
    "    \"\"\"\n",
    "    descriptions = {\n",
    "        'N': 'Normal beat',\n",
    "        'L': 'Left bundle branch block beat',\n",
    "        'R': 'Right bundle branch block beat',\n",
    "        'B': 'Bundle branch block beat (unspecified)',\n",
    "        'A': 'Atrial premature beat',\n",
    "        'a': 'Aberrated atrial premature beat',\n",
    "        'J': 'Nodal (junctional) premature beat',\n",
    "        'S': 'Supraventricular premature or ectopic beat',\n",
    "        'V': 'Premature ventricular contraction',\n",
    "        'r': 'R-on-T premature ventricular contraction',\n",
    "        'F': 'Fusion of ventricular and normal beat',\n",
    "        'e': 'Atrial escape beat',\n",
    "        'j': 'Nodal (junctional) escape beat',\n",
    "        'n': 'Supraventricular escape beat (atrial or nodal)',\n",
    "        'E': 'Ventricular escape beat',\n",
    "        '/': 'Paced beat',\n",
    "        'f': 'Fusion of paced and normal beat',\n",
    "        'Q': 'Unclassifiable beat',\n",
    "        '?': 'Beat not classified during learning',\n",
    "        '[': 'Start of ventricular flutter/fibrillation',\n",
    "        ']': 'End of ventricular flutter/fibrillation',\n",
    "        '!': 'Ventricular flutter wave',\n",
    "        'x': 'Non-conducted P-wave (blocked APC)',\n",
    "        '(': 'Waveform onset',\n",
    "        ')': 'Waveform end',\n",
    "        'p': 'Peak of P-wave',\n",
    "        't': 'Peak of T-wave',\n",
    "        'u': 'Peak of U-wave',\n",
    "        '`': 'PQ junction',\n",
    "        '\\'': 'J-point',\n",
    "        '^': 'Non-captured pacemaker artifact',\n",
    "        '|': 'Isolated QRS-like artifact',\n",
    "        '~': 'Change in signal quality',\n",
    "        '+': 'Rhythm change',\n",
    "        's': 'ST segment change',\n",
    "        'T': 'T-wave change',\n",
    "        '*': 'Systole',\n",
    "        'D': 'Diastole',\n",
    "        '=': 'Measurement annotation',\n",
    "        '\"': 'Comment annotation',\n",
    "        '@': 'Link to external data'\n",
    "    }\n",
    "    return descriptions.get(symbol, 'Unknown annotation type')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set data directory\n",
    "    data_dir = 'src/data/mit-bih-arrhythmia-database-1.0.0'\n",
    "    \n",
    "    # Load all records\n",
    "    print(\"Loading ECG database...\")\n",
    "    database = load_mit_bih_records(data_dir)\n",
    "    \n",
    "    # Print basic information\n",
    "    print(f\"\\nLoaded {database['metadata']['total_records']} records from RECORDS file\")\n",
    "    \n",
    "    # Get global annotation statistics\n",
    "    print(\"\\nGlobal Annotation Statistics:\")\n",
    "    annotation_stats = get_global_annotation_counts(database)\n",
    "    pd.set_option('display.max_rows', None)  # Show all rows\n",
    "    print(annotation_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70cc0ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing label distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [00:03<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common labels: {'R', 'V', 'N', '/', 'L'}\n",
      "Label frequencies: [('N', 75052), ('L', 8075), ('R', 7259), ('V', 7130), ('/', 7028)]\n",
      "\n",
      "Collecting samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [00:11<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset label distribution:\n",
      "Label /: 800 samples\n",
      "Label L: 800 samples\n",
      "Label N: 800 samples\n",
      "Label V: 800 samples\n",
      "Label R: 800 samples\n",
      "\n",
      "Dataset label distribution:\n",
      "Label L: 200 samples\n",
      "Label R: 200 samples\n",
      "Label V: 200 samples\n",
      "Label /: 200 samples\n",
      "Label N: 200 samples\n",
      "Training samples: 4000\n",
      "Validation samples: 1000\n",
      "\n",
      "Input shape: torch.Size([1, 513, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████████████████████████████████████████████| 125/125 [00:07<00:00, 17.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "Train Loss: 0.7110, Train Acc: 72.97%\n",
      "Val Loss: 0.3538, Val Acc: 88.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████| 125/125 [00:07<00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "Train Loss: 0.3308, Train Acc: 88.70%\n",
      "Val Loss: 0.3983, Val Acc: 87.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████| 125/125 [00:06<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "Train Loss: 0.2631, Train Acc: 90.60%\n",
      "Val Loss: 0.2169, Val Acc: 92.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████| 125/125 [00:06<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "Train Loss: 0.2104, Train Acc: 93.12%\n",
      "Val Loss: 0.2322, Val Acc: 91.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████| 125/125 [00:07<00:00, 17.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "Train Loss: 0.1887, Train Acc: 93.50%\n",
      "Val Loss: 0.1829, Val Acc: 93.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████████████████████████████████████████████| 125/125 [00:07<00:00, 17.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:\n",
      "Train Loss: 0.1764, Train Acc: 93.83%\n",
      "Val Loss: 0.2825, Val Acc: 91.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████████████████████████████████████████████| 125/125 [00:07<00:00, 17.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:\n",
      "Train Loss: 0.1388, Train Acc: 95.10%\n",
      "Val Loss: 0.2093, Val Acc: 93.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████████████████████████████████████████████| 125/125 [00:07<00:00, 17.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:\n",
      "Train Loss: 0.1261, Train Acc: 95.55%\n",
      "Val Loss: 0.1772, Val Acc: 93.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████████████████████████████████████████████| 125/125 [00:06<00:00, 17.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9:\n",
      "Train Loss: 0.1302, Train Acc: 95.38%\n",
      "Val Loss: 0.2541, Val Acc: 91.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████████████████████████████████████████████| 125/125 [00:07<00:00, 17.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:\n",
      "Train Loss: 0.1210, Train Acc: 95.62%\n",
      "Val Loss: 0.2066, Val Acc: 94.40%\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import wfdb\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def prepare_data(data_dir, top_n_labels=5, max_samples_per_class=1000):\n",
    "    \"\"\"Prepare balanced ECG datasets with consistent labels across splits\"\"\"\n",
    "    \n",
    "    # First, collect all samples across all records\n",
    "    all_samples = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Read record numbers\n",
    "    with open(os.path.join(data_dir, 'RECORDS'), 'r') as f:\n",
    "        record_numbers = [line.strip() for line in f]\n",
    "    \n",
    "    # First pass: collect all labels to find most common\n",
    "    print(\"Analyzing label distribution...\")\n",
    "    label_counter = Counter()\n",
    "    for record_num in tqdm(record_numbers):\n",
    "        record_path = os.path.join(data_dir, str(record_num))\n",
    "        try:\n",
    "            annotations = wfdb.rdann(record_path, 'atr')\n",
    "            label_counter.update(annotations.symbol)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Get top N most common labels\n",
    "    common_labels = set([label for label, _ in label_counter.most_common(top_n_labels)])\n",
    "    print(f\"\\nMost common labels: {common_labels}\")\n",
    "    print(\"Label frequencies:\", \n",
    "          [(label, count) for label, count in label_counter.most_common(top_n_labels)])\n",
    "    \n",
    "    # Second pass: collect samples for common labels\n",
    "    print(\"\\nCollecting samples...\")\n",
    "    for record_num in tqdm(record_numbers):\n",
    "        record_path = os.path.join(data_dir, str(record_num))\n",
    "        try:\n",
    "            record = wfdb.rdrecord(record_path)\n",
    "            annotations = wfdb.rdann(record_path, 'atr')\n",
    "            \n",
    "            # Get signal from first channel\n",
    "            signal_data = record.p_signal[:, 0]\n",
    "            \n",
    "            # Create spectrogram\n",
    "            frequencies, times, Sxx = signal.spectrogram(\n",
    "                signal_data,\n",
    "                fs=record.fs,\n",
    "                window='hann',\n",
    "                nperseg=1024,\n",
    "                noverlap=512,\n",
    "                detrend='constant'\n",
    "            )\n",
    "            \n",
    "            # Log scale and normalize spectrogram\n",
    "            Sxx = np.log1p(Sxx)\n",
    "            Sxx = (Sxx - Sxx.mean()) / (Sxx.std() + 1e-8)\n",
    "            \n",
    "            # Collect samples for common labels\n",
    "            for i in range(len(times)):\n",
    "                time_point = times[i]\n",
    "                ann_idx = np.searchsorted(annotations.sample / record.fs, time_point)\n",
    "                if ann_idx < len(annotations.symbol):\n",
    "                    label = annotations.symbol[ann_idx]\n",
    "                    if label in common_labels:\n",
    "                        all_samples.append(Sxx[:, i].reshape(Sxx.shape[0], 1))\n",
    "                        all_labels.append(label)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing record {record_num}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_samples = np.array(all_samples)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Balance classes\n",
    "    balanced_samples = []\n",
    "    balanced_labels = []\n",
    "    \n",
    "    for label in common_labels:\n",
    "        mask = all_labels == label\n",
    "        label_samples = all_samples[mask]\n",
    "        \n",
    "        # Limit samples per class\n",
    "        n_samples = min(len(label_samples), max_samples_per_class)\n",
    "        indices = np.random.choice(len(label_samples), n_samples, replace=False)\n",
    "        \n",
    "        balanced_samples.append(label_samples[indices])\n",
    "        balanced_labels.extend([label] * n_samples)\n",
    "    \n",
    "    balanced_samples = np.concatenate(balanced_samples, axis=0)\n",
    "    balanced_labels = np.array(balanced_labels)\n",
    "    \n",
    "    # Split into train and validation\n",
    "    indices = np.arange(len(balanced_labels))\n",
    "    train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=balanced_labels)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ECGDataset(balanced_samples[train_idx], balanced_labels[train_idx])\n",
    "    val_dataset = ECGDataset(balanced_samples[val_idx], balanced_labels[val_idx])\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(labels)\n",
    "        \n",
    "        # Print distribution\n",
    "        label_counts = Counter(self.labels)\n",
    "        print(\"\\nDataset label distribution:\")\n",
    "        for label_idx, count in label_counts.items():\n",
    "            label_name = self.label_encoder.inverse_transform([label_idx])[0]\n",
    "            print(f\"Label {label_name}: {count} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.FloatTensor(self.samples[idx])\n",
    "        label = torch.LongTensor([self.labels[idx]])[0]\n",
    "        return sample.unsqueeze(0), label\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=1):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block - careful with padding to handle the 1D-like input\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=(7, 3), padding=(3, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(16, 32, kernel_size=(5, 3), padding=(2, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling to handle variable input sizes\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((8, 1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    \"\"\"Compute class weights based on the inverse frequency of each class.\"\"\"\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    num_classes = len(class_counts)\n",
    "    \n",
    "    weights = {cls: total_samples / (num_classes * count) for cls, count in class_counts.items()}\n",
    "    \n",
    "    # Convert to tensor\n",
    "    class_weights = torch.tensor([weights[i] for i in range(num_classes)], dtype=torch.float)\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, device='cuda'):\n",
    "    # Compute class weights from the dataset\n",
    "    all_labels = [label.item() for _, label in train_loader.dataset]\n",
    "    class_weights = compute_class_weights(all_labels).to(device)\n",
    "    \n",
    "    # Use weighted loss function\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (spectrograms, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(spectrograms)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for spectrograms, labels in val_loader:\n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                outputs = model(spectrograms)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f'\\nEpoch {epoch+1}:')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {100.*train_correct/train_total:.2f}%')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {100.*val_correct/val_total:.2f}%')\n",
    "        \n",
    "        # Save best model\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set data directory\n",
    "    data_dir = 'src/data/mit-bih-arrhythmia-database-1.0.0'\n",
    "    \n",
    "    # Prepare datasets with top 5 most common labels\n",
    "    train_dataset, val_dataset = prepare_data(data_dir, top_n_labels=5)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_classes = len(train_dataset.label_encoder.classes_)\n",
    "    model = SimpleCNN(num_classes)\n",
    "    \n",
    "    # Print model summary\n",
    "    sample_data, _ = train_dataset[0]\n",
    "    print(f\"\\nInput shape: {sample_data.shape}\")\n",
    "    \n",
    "    # Train model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_model(model, train_loader, val_loader, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d10fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing predictions: 100%|███████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 41.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ECG Classification Model Analysis ===\n",
      "\n",
      "Overall Performance:\n",
      "Accuracy: 0.9440\n",
      "\n",
      "=== Per-Class Performance ===\n",
      "              precision  recall  f1-score   support\n",
      "/              0.980392   1.000  0.990099   200.000\n",
      "L              0.979275   0.945  0.961832   200.000\n",
      "N              0.912371   0.885  0.898477   200.000\n",
      "R              0.964286   0.945  0.954545   200.000\n",
      "V              0.887324   0.945  0.915254   200.000\n",
      "accuracy       0.944000   0.944  0.944000     0.944\n",
      "macro avg      0.944730   0.944  0.944042  1000.000\n",
      "weighted avg   0.944730   0.944  0.944042  1000.000\n",
      "\n",
      "Top 5 Most Common Confusion Pairs:\n",
      "True: N → Predicted: V (Count: 14)\n",
      "True: V → Predicted: N (Count: 10)\n",
      "True: N → Predicted: R (Count: 6)\n",
      "True: R → Predicted: V (Count: 6)\n",
      "True: R → Predicted: N (Count: 5)\n",
      "\n",
      "Confidence Analysis:\n",
      "Average confidence for correct predictions: 0.9751\n",
      "Average confidence for incorrect predictions: 0.7657\n",
      "\n",
      "High-Confidence Mistakes:\n",
      "True: V → Predicted: N (Confidence: 1.0000)\n",
      "True: V → Predicted: N (Confidence: 1.0000)\n",
      "True: N → Predicted: V (Confidence: 0.9992)\n",
      "True: V → Predicted: N (Confidence: 0.9969)\n",
      "True: N → Predicted: V (Confidence: 0.9964)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_model_predictions(model, data_loader, device, label_encoder):\n",
    "    \"\"\"\n",
    "    Analyze model predictions and generate detailed diagnostics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    incorrect_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (spectrograms, labels) in enumerate(tqdm(data_loader, desc=\"Analyzing predictions\")):\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probabilities.cpu().numpy())\n",
    "            \n",
    "            # Store incorrect predictions for analysis\n",
    "            incorrect_mask = preds != labels\n",
    "            if incorrect_mask.any():\n",
    "                incorrect_indices = torch.where(incorrect_mask)[0]\n",
    "                for idx in incorrect_indices:\n",
    "                    incorrect_samples.append({\n",
    "                        'true_label': labels[idx].item(),\n",
    "                        'predicted_label': preds[idx].item(),\n",
    "                        'confidence': probabilities[idx][preds[idx]].item(),\n",
    "                        'batch_idx': batch_idx,\n",
    "                        'sample_idx': idx.item()\n",
    "                    })\n",
    "    \n",
    "    return analyze_results(all_preds, all_labels, all_probs, incorrect_samples, label_encoder)\n",
    "\n",
    "def analyze_results(all_preds, all_labels, all_probs, incorrect_samples, label_encoder):\n",
    "    \"\"\"\n",
    "    Generate comprehensive analysis of model predictions.\n",
    "    \"\"\"\n",
    "    # Convert numerical labels to original classes and ensure they're regular Python strings\n",
    "    class_names = [str(name) for name in label_encoder.classes_]\n",
    "    true_classes = [class_names[label] for label in all_labels]\n",
    "    pred_classes = [class_names[pred] for pred in all_preds]\n",
    "    \n",
    "    # 1. Generate confusion matrix\n",
    "    cm = confusion_matrix(true_classes, pred_classes)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Generate classification report\n",
    "    report = classification_report(true_classes, pred_classes, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # 3. Analyze prediction confidence\n",
    "    confidence_analysis = {\n",
    "        'correct_conf': [],\n",
    "        'incorrect_conf': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(all_preds)):\n",
    "        conf = all_probs[i][all_preds[i]]\n",
    "        if all_preds[i] == all_labels[i]:\n",
    "            confidence_analysis['correct_conf'].append(conf)\n",
    "        else:\n",
    "            confidence_analysis['incorrect_conf'].append(conf)\n",
    "    \n",
    "    # Plot confidence distributions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(confidence_analysis['correct_conf'], alpha=0.5, label='Correct', bins=50)\n",
    "    plt.hist(confidence_analysis['incorrect_conf'], alpha=0.5, label='Incorrect', bins=50)\n",
    "    plt.title('Prediction Confidence Distribution')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confidence_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Analyze most common confusion pairs\n",
    "    confusion_pairs = []\n",
    "    for sample in incorrect_samples:\n",
    "        true_class = class_names[sample['true_label']]\n",
    "        pred_class = class_names[sample['predicted_label']]\n",
    "        confusion_pairs.append((true_class, pred_class))\n",
    "    \n",
    "    # Handle confusion pairs\n",
    "    if confusion_pairs:  # Only process if there are incorrect predictions\n",
    "        confusion_series = pd.Series(confusion_pairs)\n",
    "        confusion_values = confusion_series.value_counts()\n",
    "        confusion_counts = pd.DataFrame([\n",
    "            {'True Class': str(true), 'Predicted Class': str(pred), 'Count': count}\n",
    "            for (true, pred), count in confusion_values.items()\n",
    "        ])\n",
    "    else:\n",
    "        confusion_counts = pd.DataFrame(columns=['True Class', 'Predicted Class', 'Count'])\n",
    "    \n",
    "    # 5. Generate summary statistics\n",
    "    class_performance = {}\n",
    "    for class_name in class_names:\n",
    "        str_class_name = str(class_name)\n",
    "        if str_class_name in report:\n",
    "            class_performance[str_class_name] = {\n",
    "                'precision': report[str_class_name]['precision'],\n",
    "                'recall': report[str_class_name]['recall'],\n",
    "                'f1-score': report[str_class_name]['f1-score'],\n",
    "                'support': report[str_class_name]['support']\n",
    "            }\n",
    "    \n",
    "    summary_stats = {\n",
    "        'Overall Accuracy': report['accuracy'],\n",
    "        'Most Confused Pairs': confusion_counts.head(5).to_dict('records'),\n",
    "        'Per Class Performance': class_performance,\n",
    "        'Average Confidence': {\n",
    "            'Correct Predictions': np.mean(confidence_analysis['correct_conf']) if confidence_analysis['correct_conf'] else 0.0,\n",
    "            'Incorrect Predictions': np.mean(confidence_analysis['incorrect_conf']) if confidence_analysis['incorrect_conf'] else 0.0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report_df,\n",
    "        'confidence_analysis': confidence_analysis,\n",
    "        'confusion_pairs': confusion_counts,\n",
    "        'summary_stats': summary_stats,\n",
    "        'incorrect_samples': incorrect_samples\n",
    "    }\n",
    "\n",
    "def print_analysis_results(results):\n",
    "    \"\"\"\n",
    "    Print formatted analysis results.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== ECG Classification Model Analysis ===\\n\")\n",
    "    \n",
    "    # Overall Performance\n",
    "    print(\"Overall Performance:\")\n",
    "    print(f\"Accuracy: {results['summary_stats']['Overall Accuracy']:.4f}\")\n",
    "    print(\"\\n=== Per-Class Performance ===\")\n",
    "    print(results['classification_report'])\n",
    "    \n",
    "    # Most Common Confusion Pairs\n",
    "    print(\"\\nTop 5 Most Common Confusion Pairs:\")\n",
    "    for pair in results['summary_stats']['Most Confused Pairs']:\n",
    "        print(f\"True: {pair['True Class']} → Predicted: {pair['Predicted Class']} \"\n",
    "              f\"(Count: {pair['Count']})\")\n",
    "    \n",
    "    # Confidence Analysis\n",
    "    print(\"\\nConfidence Analysis:\")\n",
    "    print(f\"Average confidence for correct predictions: \"\n",
    "          f\"{results['summary_stats']['Average Confidence']['Correct Predictions']:.4f}\")\n",
    "    print(f\"Average confidence for incorrect predictions: \"\n",
    "          f\"{results['summary_stats']['Average Confidence']['Incorrect Predictions']:.4f}\")\n",
    "    \n",
    "    # Examples of High-Confidence Mistakes\n",
    "    print(\"\\nHigh-Confidence Mistakes:\")\n",
    "    high_conf_mistakes = sorted(\n",
    "        [x for x in results['incorrect_samples'] if x['confidence'] > 0.8],\n",
    "        key=lambda x: x['confidence'],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "    \n",
    "    for mistake in high_conf_mistakes:\n",
    "        true_label = results['classification_report'].index[mistake['true_label']]\n",
    "        pred_label = results['classification_report'].index[mistake['predicted_label']]\n",
    "        print(f\"True: {true_label} → Predicted: {pred_label} \"\n",
    "              f\"(Confidence: {mistake['confidence']:.4f})\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming model, data_loader, and label_encoder are defined\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    results = analyze_model_predictions(model, val_loader, device, train_dataset.label_encoder)\n",
    "    print_analysis_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341a193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
